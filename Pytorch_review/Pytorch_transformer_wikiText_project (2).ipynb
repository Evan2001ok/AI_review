{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swWpr0JFIS0L"
      },
      "outputs": [],
      "source": [
        "#WikiText序列预测项目\n",
        "#步骤 1：导入必要的工具包\n",
        "#步骤 2：导入 wikiText-2 数据集并进行基本处理\n",
        "#步骤 3：根据模型输入构建批量数据\n",
        "#步骤 4：构建训练和评估函数\n",
        "#步骤 5：进行训练和评估（包括验证和测试）"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TorchText version: {torchtext.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE231RbAmpnd",
        "outputId": "d79438d8-65ab-4c99-88f9-d410725a2bda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.2.0+cu121\n",
            "TorchText version: 0.17.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#从huggingface下载训练集，解压\n",
        "!unzip archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSFEZl8V5VnD",
        "outputId": "62b652fb-d250-46f0-b1a7-cf4d8d6ddc8f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive.zip\n",
            "  inflating: wikitext-2/wiki.test.tokens  \n",
            "  inflating: wikitext-2/wiki.train.tokens  \n",
            "  inflating: wikitext-2/wiki.valid.tokens  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#预处理wikitext2数据集\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 加载分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def process_file(filename, split_name):\n",
        "    print(f\"正在处理 {filename}...\")\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # 过滤空行和标题行（以=开头的行）\n",
        "    texts = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line and not line.startswith('=') and len(line) > 5:\n",
        "            texts.append(line)\n",
        "\n",
        "    print(f\"{split_name}集有效行数: {len(texts)}\")\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    batch_size = 500\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        all_input_ids.append(encoded[\"input_ids\"])\n",
        "        all_attention_masks.append(encoded[\"attention_mask\"])\n",
        "\n",
        "        if (i // batch_size) % 10 == 0:  # 每10个batch显示一次进度\n",
        "            print(f\"{split_name} - 已处理: {min(i+batch_size, len(texts))}/{len(texts)}\")\n",
        "\n",
        "    return torch.cat(all_input_ids, dim=0), torch.cat(all_attention_masks, dim=0)\n",
        "\n",
        "# 处理三个文件\n",
        "print(\"开始处理WikiText-2数据集...\")\n",
        "\n",
        "train_ids, train_masks = process_file(\"wikitext-2/wiki.train.tokens\", \"训练\")\n",
        "val_ids, val_masks = process_file(\"wikitext-2/wiki.valid.tokens\", \"验证\")\n",
        "test_ids, test_masks = process_file(\"wikitext-2/wiki.test.tokens\", \"测试\")\n",
        "\n",
        "# 保存处理结果\n",
        "torch.save({\n",
        "    \"train_input_ids\": train_ids,\n",
        "    \"train_attention_mask\": train_masks,\n",
        "    \"val_input_ids\": val_ids,\n",
        "    \"val_attention_mask\": val_masks,\n",
        "    \"test_input_ids\": test_ids,\n",
        "    \"test_attention_mask\": test_masks\n",
        "}, \"wikitext2_processed.pt\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"WikiText-2 数据预处理完成！\")\n",
        "print(\"=\"*50)\n",
        "print(f\"训练集形状: {train_ids.shape}\")\n",
        "print(f\"验证集形状: {val_ids.shape}\")\n",
        "print(f\"测试集形状: {test_ids.shape}\")\n",
        "print(f\"数据已保存到: wikitext2_processed.pt\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 显示一些统计信息\n",
        "print(f\"\\n数据统计:\")\n",
        "print(f\"- 训练样本数: {train_ids.shape[0]:,}\")\n",
        "print(f\"- 验证样本数: {val_ids.shape[0]:,}\")\n",
        "print(f\"- 测试样本数: {test_ids.shape[0]:,}\")\n",
        "print(f\"- 序列长度: {train_ids.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lccQCWN5qBq",
        "outputId": "5f50b66d-aca4-4e30-fbaa-0dd78acc3f15"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始处理WikiText-2数据集...\n",
            "正在处理 wikitext-2/wiki.train.tokens...\n",
            "训练集有效行数: 17456\n",
            "训练 - 已处理: 500/17456\n",
            "训练 - 已处理: 5500/17456\n",
            "训练 - 已处理: 10500/17456\n",
            "训练 - 已处理: 15500/17456\n",
            "正在处理 wikitext-2/wiki.valid.tokens...\n",
            "验证集有效行数: 1841\n",
            "验证 - 已处理: 500/1841\n",
            "正在处理 wikitext-2/wiki.test.tokens...\n",
            "测试集有效行数: 2163\n",
            "测试 - 已处理: 500/2163\n",
            "\n",
            "==================================================\n",
            "WikiText-2 数据预处理完成！\n",
            "==================================================\n",
            "训练集形状: torch.Size([17456, 128])\n",
            "验证集形状: torch.Size([1841, 128])\n",
            "测试集形状: torch.Size([2163, 128])\n",
            "数据已保存到: wikitext2_processed.pt\n",
            "==================================================\n",
            "\n",
            "数据统计:\n",
            "- 训练样本数: 17,456\n",
            "- 验证样本数: 1,841\n",
            "- 测试样本数: 2,163\n",
            "- 序列长度: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#根据模型输入构建批量数据\n",
        "import torch\n",
        "\n",
        "def create_batches(data_path='wikitext2_processed.pt', batch_size=20, seq_len=35):\n",
        "    \"\"\"\n",
        "    最简单的批量数据创建\n",
        "    \"\"\"\n",
        "    # 加载数据\n",
        "    data = torch.load(data_path)\n",
        "\n",
        "    def process_data(input_ids):\n",
        "        # 展平所有序列，移除padding\n",
        "        tokens = []\n",
        "        for seq in input_ids:\n",
        "            valid_tokens = seq[seq != 0]  # 移除padding\n",
        "            tokens.append(valid_tokens)\n",
        "\n",
        "        # 合并所有token\n",
        "        all_tokens = torch.cat(tokens, dim=0)\n",
        "\n",
        "        # 批量化：[总长度] -> [seq_len, batch_size]\n",
        "        nbatch = all_tokens.size(0) // batch_size\n",
        "        all_tokens = all_tokens[:nbatch * batch_size]\n",
        "        return all_tokens.view(batch_size, -1).t()\n",
        "\n",
        "    # 处理三个数据集\n",
        "    train_data = process_data(data['train_input_ids'])\n",
        "    val_data = process_data(data['val_input_ids'])\n",
        "    test_data = process_data(data['test_input_ids'])\n",
        "\n",
        "    # 获取词汇表大小\n",
        "    vocab_size = 30522  # BERT tokenizer的词汇表大小\n",
        "\n",
        "    print(f\"训练数据: {train_data.shape}\")\n",
        "    print(f\"验证数据: {val_data.shape}\")\n",
        "    print(f\"测试数据: {test_data.shape}\")\n",
        "\n",
        "    return train_data, val_data, test_data, vocab_size\n",
        "\n",
        "def get_batch(source, i, seq_len=35):\n",
        "    \"\"\"\n",
        "    获取一个批次的数据\n",
        "    \"\"\"\n",
        "    seq_len = min(seq_len, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]           # 输入\n",
        "    target = source[i+1:i+1+seq_len]     # 目标（下一个词）\n",
        "    return data, target.reshape(-1)\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 创建批量数据\n",
        "    train_data, val_data, test_data, vocab_size = create_batches()\n",
        "\n",
        "    # 获取一个批次测试\n",
        "    data, targets = get_batch(train_data, 0)\n",
        "    print(f\"批次输入: {data.shape}\")\n",
        "    print(f\"批次目标: {targets.shape}\")\n",
        "\n",
        "    print(\"✅ 完成！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGQ5W9Tz91g7",
        "outputId": "564687bc-68fc-4366-d80c-0e905d6556e6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练数据: torch.Size([84773, 20])\n",
            "验证数据: torch.Size([9058, 20])\n",
            "测试数据: torch.Size([10265, 20])\n",
            "批次输入: torch.Size([35, 20])\n",
            "批次目标: torch.Size([700])\n",
            "✅ 完成！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#获取官方源码\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "vUDnkzxz83Fd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#设置超参数和构建模型\n",
        "ntoken = vocab_size # vocab_size是因为用的是bert分词生成的字典\n",
        "ninp = 200 #词嵌入维度\n",
        "nhead = 2\n",
        "nhid = 200 #前馈全连接层节点数\n",
        "nlayers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "#设置cuda设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#按照超参数创建模型对象\n",
        "model = TransformerModel(ntoken=ntoken,ninp=ninp,nhead=nhead,nhid=nhid,nlayers=nlayers,dropout=dropout).to(device)\n",
        "\n",
        "#准备训练\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # 学习率\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr) #创建优化器\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)#学习率自动调整工具\n",
        "\n",
        "# 开始训练\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, 35)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntoken), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch} finished, Avg Loss: {total_loss/batch:.4f}')\n",
        "\n",
        "print(\"训练完成！\")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "b1VJv3fb-Lu_",
        "outputId": "2c7ae480-7792-4f76-8272-5ddc9dc4c42a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 10.6197\n",
            "Epoch 0, Batch 100, Loss: 6.8308\n",
            "Epoch 0, Batch 200, Loss: 6.4037\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-da726eee8939>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 获取正确的词汇表大小\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")# 获取bert分词器\n",
        "vocab_size = tokenizer.vocab_size  # 30522\n",
        "\n",
        "# 设置超参数和构建模型\n",
        "ntoken = vocab_size # BERT分词器的词汇表大小\n",
        "ninp = 200 #词嵌入维度\n",
        "nhead = 2\n",
        "nhid = 200 #前馈全连接层节点数\n",
        "nlayers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# 设置cuda设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用设备: {device}\")\n",
        "\n",
        "# 按照超参数创建模型对象\n",
        "model = TransformerModel(ntoken=ntoken,ninp=ninp,nhead=nhead,nhid=nhid,nlayers=nlayers,dropout=dropout).to(device)\n",
        "\n",
        "# 将数据移到设备上\n",
        "train_data = train_data.to(device)\n",
        "val_data = val_data.to(device)\n",
        "test_data = test_data.to(device)\n",
        "\n",
        "# 准备训练\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # 学习率\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr) #创建优化器\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)#学习率自动调整工具\n",
        "\n",
        "def evaluate_model(model, data, device):\n",
        "    \"\"\"简单的模型评估函数\"\"\"\n",
        "    model.eval()  # 设置为评估模式\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():  # 不计算梯度，节省内存\n",
        "        for i in range(0, data.size(0) - 1, 35):\n",
        "            # 获取批次数据\n",
        "            input_data, targets = get_batch(data, i)\n",
        "            input_data, targets = input_data.to(device), targets.to(device)\n",
        "\n",
        "            # 前向传播\n",
        "            output = model(input_data)\n",
        "            loss = criterion(output.view(-1, ntoken), targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "    # 计算平均损失和困惑度\n",
        "    avg_loss = total_loss / total_batches\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "def simple_evaluate():\n",
        "    \"\"\"简单评估并打印结果\"\"\"\n",
        "    # 验证集评估\n",
        "    val_loss, val_ppl = evaluate_model(model, val_data, device)\n",
        "    print(f\"验证集 - Loss: {val_loss:.4f}, 困惑度: {val_ppl:.2f}\")\n",
        "\n",
        "    # 测试集评估\n",
        "    test_loss, test_ppl = evaluate_model(model, test_data, device)\n",
        "    print(f\"测试集 - Loss: {test_loss:.4f}, 困惑度: {test_ppl:.2f}\")\n",
        "\n",
        "    return val_loss, test_loss\n",
        "\n",
        "# 开始训练\n",
        "print(\"开始训练...\")\n",
        "print(f\"训练数据形状: {train_data.shape}\")\n",
        "print(f\"总批次数: {(train_data.size(0) - 1) // 35}\")\n",
        "\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_batches = (train_data.size(0) - 1) // 35\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, 35)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntoken), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch}/{total_batches}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = total_loss / (batch + 1)\n",
        "    print(f'Epoch {epoch} finished, Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # 每个epoch后评估\n",
        "    print(f\"\\nEpoch {epoch} 评估结果:\")\n",
        "    val_loss, test_loss = simple_evaluate()\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 更新学习率\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"训练完成！\")\n",
        "\n",
        "# 最终评估\n",
        "print(\"\\n=== 最终评估结果 ===\")\n",
        "simple_evaluate()\n",
        "\n",
        "# 保存模型（可选）\n",
        "torch.save(model.state_dict(), 'wikitext_transformer_model.pt')\n",
        "print(\"模型已保存到: wikitext_transformer_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIzneEPpPlTj",
        "outputId": "e50fc545-252e-44a0-893d-fc716b8cf42b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用设备: cpu\n",
            "开始训练...\n",
            "训练数据形状: torch.Size([84773, 20])\n",
            "总批次数: 2422\n",
            "Epoch 0, Batch 0/2422, Loss: 10.6549\n",
            "Epoch 0, Batch 100/2422, Loss: 6.6222\n",
            "Epoch 0, Batch 200/2422, Loss: 6.4848\n",
            "Epoch 0, Batch 300/2422, Loss: 6.3450\n",
            "Epoch 0, Batch 400/2422, Loss: 5.9682\n",
            "Epoch 0, Batch 500/2422, Loss: 6.0599\n",
            "Epoch 0, Batch 600/2422, Loss: 6.2041\n",
            "Epoch 0, Batch 700/2422, Loss: 5.9127\n",
            "Epoch 0, Batch 800/2422, Loss: 5.9831\n",
            "Epoch 0, Batch 900/2422, Loss: 5.7636\n",
            "Epoch 0, Batch 1000/2422, Loss: 5.6051\n",
            "Epoch 0, Batch 1100/2422, Loss: 5.5765\n",
            "Epoch 0, Batch 1200/2422, Loss: 6.1256\n",
            "Epoch 0, Batch 1300/2422, Loss: 5.9243\n",
            "Epoch 0, Batch 1400/2422, Loss: 5.3704\n",
            "Epoch 0, Batch 1500/2422, Loss: 5.7416\n",
            "Epoch 0, Batch 1600/2422, Loss: 5.4538\n",
            "Epoch 0, Batch 1700/2422, Loss: 5.3165\n",
            "Epoch 0, Batch 1800/2422, Loss: 5.2734\n",
            "Epoch 0, Batch 1900/2422, Loss: 5.2922\n",
            "Epoch 0, Batch 2000/2422, Loss: 5.4741\n",
            "Epoch 0, Batch 2100/2422, Loss: 5.6420\n",
            "Epoch 0, Batch 2200/2422, Loss: 5.2162\n",
            "Epoch 0, Batch 2300/2422, Loss: 5.6071\n",
            "Epoch 0, Batch 2400/2422, Loss: 5.6880\n",
            "Epoch 0 finished, Avg Loss: 5.8058\n",
            "\n",
            "Epoch 0 评估结果:\n",
            "验证集 - Loss: 4.9739, 困惑度: 144.60\n",
            "测试集 - Loss: 4.8062, 困惑度: 122.27\n",
            "--------------------------------------------------\n",
            "Epoch 1, Batch 0/2422, Loss: 5.7017\n",
            "Epoch 1, Batch 100/2422, Loss: 5.2987\n",
            "Epoch 1, Batch 200/2422, Loss: 5.2382\n",
            "Epoch 1, Batch 300/2422, Loss: 5.3472\n",
            "Epoch 1, Batch 400/2422, Loss: 5.1014\n",
            "Epoch 1, Batch 500/2422, Loss: 5.3375\n",
            "Epoch 1, Batch 600/2422, Loss: 5.4173\n",
            "Epoch 1, Batch 700/2422, Loss: 5.2247\n",
            "Epoch 1, Batch 800/2422, Loss: 5.2116\n",
            "Epoch 1, Batch 900/2422, Loss: 5.2185\n",
            "Epoch 1, Batch 1000/2422, Loss: 4.9990\n",
            "Epoch 1, Batch 1100/2422, Loss: 5.0697\n",
            "Epoch 1, Batch 1200/2422, Loss: 5.6815\n",
            "Epoch 1, Batch 1300/2422, Loss: 5.5103\n",
            "Epoch 1, Batch 1400/2422, Loss: 4.8138\n",
            "Epoch 1, Batch 1500/2422, Loss: 5.1517\n",
            "Epoch 1, Batch 1600/2422, Loss: 5.0284\n",
            "Epoch 1, Batch 1700/2422, Loss: 4.8172\n",
            "Epoch 1, Batch 1800/2422, Loss: 4.8918\n",
            "Epoch 1, Batch 1900/2422, Loss: 4.7609\n",
            "Epoch 1, Batch 2000/2422, Loss: 5.1416\n",
            "Epoch 1, Batch 2100/2422, Loss: 5.0730\n",
            "Epoch 1, Batch 2200/2422, Loss: 4.9212\n",
            "Epoch 1, Batch 2300/2422, Loss: 5.1281\n",
            "Epoch 1, Batch 2400/2422, Loss: 5.2909\n",
            "Epoch 1 finished, Avg Loss: 5.1318\n",
            "\n",
            "Epoch 1 评估结果:\n",
            "验证集 - Loss: 4.8674, 困惑度: 129.98\n",
            "测试集 - Loss: 4.6981, 困惑度: 109.73\n",
            "--------------------------------------------------\n",
            "训练完成！\n",
            "\n",
            "=== 最终评估结果 ===\n",
            "验证集 - Loss: 4.8674, 困惑度: 129.98\n",
            "测试集 - Loss: 4.6981, 困惑度: 109.73\n",
            "模型已保存到: wikitext_transformer_model.pt\n"
          ]
        }
      ]
    }
  ]
}