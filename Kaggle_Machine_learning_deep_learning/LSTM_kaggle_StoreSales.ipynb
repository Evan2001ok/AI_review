{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Kaggleçš„Store Sales - Time Series Forecastingä»»åŠ¡"
      ],
      "metadata": {
        "id": "zQRBG9G3N5r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip store-sales-time-series-forecasting.zip && rm -rf store-sales-time-series-forecasting.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzXNop4Am05k",
        "outputId": "16a779c5-3d3d-43e9-af3c-a3c1378049c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  store-sales-time-series-forecasting.zip\n",
            "  inflating: holidays_events.csv     \n",
            "  inflating: oil.csv                 \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n",
            "  inflating: transactions.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def simple_preprocess(df):\n",
        "    \"\"\"æ•°æ®é¢„å¤„ç†\"\"\"\n",
        "    df = df.copy()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.dropna(subset=['sales'])\n",
        "    df = df[df['sales'] >= 0]\n",
        "    df = df.sort_values(['store_nbr', 'family', 'date'])\n",
        "    return df\n",
        "\n",
        "def create_simple_features(df):\n",
        "    \"\"\"å¢å¼ºç‰¹å¾å·¥ç¨‹\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # åŸºç¡€æ—¶é—´ç‰¹å¾\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['dayofweek'] = df['date'].dt.dayofweek\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['quarter'] = df['date'].dt.quarter\n",
        "    df['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)\n",
        "    df['is_month_start'] = (df['date'].dt.day <= 5).astype(int)\n",
        "    df['is_month_end'] = (df['date'].dt.day >= 25).astype(int)\n",
        "\n",
        "    # ä¿ƒé”€ç‰¹å¾\n",
        "    df['onpromotion'] = df['onpromotion'].fillna(0)\n",
        "\n",
        "    # å¤šä¸ªæ»åç‰¹å¾ - ä¿®å¤ç´¢å¼•é—®é¢˜\n",
        "    df = df.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
        "    for lag in [1, 7, 14, 28]:\n",
        "        df[f'sales_lag{lag}'] = df.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
        "\n",
        "    # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ - ä¿®å¤ç´¢å¼•é—®é¢˜\n",
        "    for window in [7, 14, 28]:\n",
        "        # è®¡ç®—æ»šåŠ¨å‡å€¼\n",
        "        rolling_mean = df.groupby(['store_nbr', 'family'])['sales'].rolling(window, min_periods=1).mean()\n",
        "        df[f'sales_rolling_mean_{window}'] = rolling_mean.values\n",
        "\n",
        "        # è®¡ç®—æ»šåŠ¨æ ‡å‡†å·®\n",
        "        rolling_std = df.groupby(['store_nbr', 'family'])['sales'].rolling(window, min_periods=1).std()\n",
        "        df[f'sales_rolling_std_{window}'] = rolling_std.values\n",
        "\n",
        "    # ä¿ƒé”€ç›¸å…³ç‰¹å¾\n",
        "    df['promo_lag1'] = df.groupby(['store_nbr', 'family'])['onpromotion'].shift(1)\n",
        "    promo_rolling = df.groupby(['store_nbr', 'family'])['onpromotion'].rolling(7, min_periods=1).sum()\n",
        "    df['promo_rolling_sum_7'] = promo_rolling.values\n",
        "\n",
        "    df = df.fillna(0)\n",
        "    return df\n",
        "\n",
        "def create_sequences_simple(df, seq_len=14, pred_len=15):\n",
        "    \"\"\"åˆ›å»ºæ—¶é—´åºåˆ—ç”¨äºå¤šå¤©é¢„æµ‹\"\"\"\n",
        "    feature_cols = ['month', 'dayofweek', 'day', 'quarter', 'is_weekend', 'is_month_start', 'is_month_end',\n",
        "                   'onpromotion', 'sales_lag1', 'sales_lag7', 'sales_lag14', 'sales_lag28',\n",
        "                   'sales_rolling_mean_7', 'sales_rolling_mean_14', 'sales_rolling_mean_28',\n",
        "                   'sales_rolling_std_7', 'sales_rolling_std_14', 'sales_rolling_std_28',\n",
        "                   'promo_lag1', 'promo_rolling_sum_7']\n",
        "    X, y = [], []\n",
        "\n",
        "    for (store, family), group in df.groupby(['store_nbr', 'family']):\n",
        "        group = group.sort_values('date')\n",
        "        if len(group) < seq_len + pred_len:\n",
        "            continue\n",
        "\n",
        "        features = group[feature_cols].values\n",
        "        sales = group['sales'].values\n",
        "\n",
        "        for i in range(len(group) - seq_len - pred_len + 1):\n",
        "            X.append(features[i:i+seq_len])\n",
        "            y.append(sales[i+seq_len:i+seq_len+pred_len])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def create_dataloaders_simple(X, y, batch_size=32):\n",
        "    \"\"\"åˆ›å»ºæ•°æ®åŠ è½½å™¨\"\"\"\n",
        "    scaler_X = StandardScaler()\n",
        "    n_samples, seq_len, n_features = X.shape\n",
        "    X_scaled = scaler_X.fit_transform(X.reshape(-1, n_features))\n",
        "    X_scaled = X_scaled.reshape(n_samples, seq_len, n_features)\n",
        "\n",
        "    scaler_y = StandardScaler()\n",
        "    y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "    split = int(0.8 * len(X_scaled))\n",
        "    X_train, X_test = X_scaled[:split], X_scaled[split:]\n",
        "    y_train, y_test = y_scaled[:split], y_scaled[split:]\n",
        "\n",
        "    X_train = torch.FloatTensor(X_train)\n",
        "    X_test = torch.FloatTensor(X_test)\n",
        "    y_train = torch.FloatTensor(y_train)\n",
        "    y_test = torch.FloatTensor(y_test)\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "    return train_loader, test_loader, scaler_X, scaler_y\n",
        "\n",
        "def process_data_simple(df):\n",
        "    \"\"\"æ€»æ•°æ®å¤„ç†æµç¨‹\"\"\"\n",
        "    df = simple_preprocess(df)\n",
        "    df = create_simple_features(df)\n",
        "    X, y = create_sequences_simple(df)\n",
        "    train_loader, test_loader, scaler_X, scaler_y = create_dataloaders_simple(X, y)\n",
        "\n",
        "    print(f\"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}\")\n",
        "    print(f\"æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset)}\")\n",
        "    print(f\"è¾“å…¥ç‰¹å¾æ•°: {X.shape[2]}\")\n",
        "    print(f\"è¾“å…¥åºåˆ—é•¿åº¦: {X.shape[1]}å¤©\")\n",
        "    print(f\"é¢„æµ‹åºåˆ—é•¿åº¦: {y.shape[1]}å¤©\")\n",
        "\n",
        "    return train_loader, test_loader, X.shape[2], y.shape[1], scaler_X, scaler_y\n",
        "\n",
        "def process_test_data(test_df, train_df, scaler_X, scaler_y):\n",
        "    \"\"\"å¤„ç†æµ‹è¯•æ•°æ®\"\"\"\n",
        "    test_df = test_df.copy()\n",
        "    test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "    test_df['onpromotion'] = test_df['onpromotion'].fillna(0)\n",
        "\n",
        "    # å¢å¼ºæ—¶é—´ç‰¹å¾\n",
        "    test_df['month'] = test_df['date'].dt.month\n",
        "    test_df['dayofweek'] = test_df['date'].dt.dayofweek\n",
        "    test_df['day'] = test_df['date'].dt.day\n",
        "    test_df['quarter'] = test_df['date'].dt.quarter\n",
        "    test_df['is_weekend'] = (test_df['date'].dt.dayofweek >= 5).astype(int)\n",
        "    test_df['is_month_start'] = (test_df['date'].dt.day <= 5).astype(int)\n",
        "    test_df['is_month_end'] = (test_df['date'].dt.day >= 25).astype(int)\n",
        "\n",
        "    # ä»è®­ç»ƒæ•°æ®è·å–å„ç§ç»Ÿè®¡ç‰¹å¾\n",
        "    last_stats = train_df.groupby(['store_nbr', 'family']).agg({\n",
        "        'sales': ['last', 'mean', 'std']\n",
        "    }).reset_index()\n",
        "    last_stats.columns = ['store_nbr', 'family', 'sales_lag1', 'sales_mean', 'sales_std']\n",
        "\n",
        "    # åˆå¹¶ç»Ÿè®¡ç‰¹å¾\n",
        "    test_df = test_df.merge(last_stats, on=['store_nbr', 'family'], how='left')\n",
        "\n",
        "    # å¡«å……ç¼ºå¤±çš„æ»åç‰¹å¾\n",
        "    for lag in [7, 14, 28]:\n",
        "        test_df[f'sales_lag{lag}'] = test_df['sales_lag1']  # ç”¨æœ€è¿‘çš„é”€å”®å€¼å¡«å……\n",
        "\n",
        "    # å¡«å……æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾\n",
        "    for window in [7, 14, 28]:\n",
        "        test_df[f'sales_rolling_mean_{window}'] = test_df['sales_mean']\n",
        "        test_df[f'sales_rolling_std_{window}'] = test_df['sales_std']\n",
        "\n",
        "    # ä¿ƒé”€ç›¸å…³ç‰¹å¾\n",
        "    test_df['promo_lag1'] = test_df['onpromotion']\n",
        "    test_df['promo_rolling_sum_7'] = test_df['onpromotion'] * 3  # å‡è®¾å¹³å‡ä¿ƒé”€é¢‘ç‡\n",
        "\n",
        "    test_df = test_df.fillna(0)\n",
        "\n",
        "    feature_cols = ['month', 'dayofweek', 'day', 'quarter', 'is_weekend', 'is_month_start', 'is_month_end',\n",
        "                   'onpromotion', 'sales_lag1', 'sales_lag7', 'sales_lag14', 'sales_lag28',\n",
        "                   'sales_rolling_mean_7', 'sales_rolling_mean_14', 'sales_rolling_mean_28',\n",
        "                   'sales_rolling_std_7', 'sales_rolling_std_14', 'sales_rolling_std_28',\n",
        "                   'promo_lag1', 'promo_rolling_sum_7']\n",
        "    X_test = []\n",
        "    all_predictions_info = []\n",
        "\n",
        "    for (store, family), group in test_df.groupby(['store_nbr', 'family']):\n",
        "        group = group.sort_values('date')\n",
        "\n",
        "        if len(group) >= 14:\n",
        "            features = group[feature_cols].iloc[:14].values\n",
        "            X_test.append(features)\n",
        "            all_ids = group['id'].values\n",
        "            all_predictions_info.append({\n",
        "                'store': store,\n",
        "                'family': family,\n",
        "                'ids': all_ids\n",
        "            })\n",
        "        else:\n",
        "            print(f\"è­¦å‘Š: åº—é“º{store}çš„{family}ç±»åˆ«æ•°æ®ä¸è¶³\")\n",
        "            features = np.zeros((14, len(feature_cols)))\n",
        "            X_test.append(features)\n",
        "            all_predictions_info.append({\n",
        "                'store': store,\n",
        "                'family': family,\n",
        "                'ids': group['id'].values\n",
        "            })\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    n_samples, seq_len, n_features = X_test.shape\n",
        "    X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features))\n",
        "    X_test_scaled = X_test_scaled.reshape(n_samples, seq_len, n_features)\n",
        "\n",
        "    return X_test_scaled, all_predictions_info\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "        if x.is_cuda:\n",
        "            h_0 = h_0.cuda()\n",
        "            c_0 = c_0.cuda()\n",
        "\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        out = self.fc(last_output)\n",
        "\n",
        "        return out\n",
        "\n",
        "def train_and_evaluate(train_loader, test_loader, n_features, pred_len, device, num_epochs=12):\n",
        "    \"\"\"è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹\"\"\"\n",
        "    model = LSTM(input_size=n_features, hidden_size=64, num_layers=2, output_size=pred_len).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # è®­ç»ƒ\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 500 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # è¯„ä¼°\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    print(f\"Test Loss: {total_loss/len(test_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # åŠ è½½æ•°æ®\n",
        "    data = pd.read_csv('train.csv')\n",
        "    test_data = pd.read_csv('test.csv')\n",
        "\n",
        "    # æ•°æ®é¢„å¤„ç†\n",
        "    train_loader, test_loader, n_features, pred_len, scaler_X, scaler_y = process_data_simple(data)\n",
        "\n",
        "    # è®­ç»ƒå’Œè¯„ä¼°\n",
        "    model = train_and_evaluate(train_loader, test_loader, n_features, pred_len, device, num_epochs=10)\n",
        "\n",
        "    # å¤„ç†æµ‹è¯•æ•°æ®å¹¶é¢„æµ‹\n",
        "    X_test, predictions_info = process_test_data(test_data, data, scaler_X, scaler_y)\n",
        "\n",
        "    # é¢„æµ‹\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions_scaled = model(X_test_tensor)\n",
        "\n",
        "    predictions = scaler_y.inverse_transform(predictions_scaled.cpu().numpy())\n",
        "\n",
        "    # ç”Ÿæˆæäº¤æ–‡ä»¶\n",
        "    submission_data = {}\n",
        "    for i, info in enumerate(predictions_info):\n",
        "        pred_15days = predictions[i]\n",
        "        ids = info['ids']\n",
        "\n",
        "        for j, test_id in enumerate(ids):\n",
        "            if j < len(pred_15days):\n",
        "                submission_data[test_id] = max(0, pred_15days[j])\n",
        "            else:\n",
        "                submission_data[test_id] = max(0, pred_15days[-1])\n",
        "\n",
        "    test_original = pd.read_csv('test.csv')\n",
        "    submission_df = test_original[['id']].copy()\n",
        "    submission_df['sales'] = submission_df['id'].map(submission_data)\n",
        "    submission_df['sales'] = submission_df['sales'].fillna(0)\n",
        "\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "    print(f\"æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {len(submission_df)} è¡Œ\")\n",
        "    print(f\"ç©ºå€¼æ•°é‡: {submission_df['sales'].isna().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM9SunkYQLTV",
        "outputId": "eae13b28-4322-46a8-a262-87fd2f5b56ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Batch 34000, Loss: 0.1242\n",
            "Epoch 10, Batch 34500, Loss: 0.1262\n",
            "Epoch 10, Batch 35000, Loss: 0.0115\n",
            "Epoch 10, Batch 35500, Loss: 0.0088\n",
            "Epoch 10, Batch 36000, Loss: 0.0082\n",
            "Epoch 10, Batch 36500, Loss: 0.0077\n",
            "Epoch 10, Batch 37000, Loss: 0.0055\n",
            "Epoch 10, Batch 37500, Loss: 0.0481\n",
            "Epoch 10, Batch 38000, Loss: 0.0720\n",
            "Epoch 10, Batch 38500, Loss: 0.0014\n",
            "Epoch 10, Batch 39000, Loss: 0.0069\n",
            "Epoch 10, Batch 39500, Loss: 0.0459\n",
            "Epoch 10, Batch 40000, Loss: 0.0165\n",
            "Epoch 10, Batch 40500, Loss: 0.0056\n",
            "Epoch 10, Batch 41000, Loss: 0.0071\n",
            "Epoch 10, Batch 41500, Loss: 0.0132\n",
            "Epoch 10, Batch 42000, Loss: 0.0084\n",
            "Epoch 10, Batch 42500, Loss: 0.0090\n",
            "Epoch 10, Batch 43000, Loss: 0.0021\n",
            "Epoch 10, Batch 43500, Loss: 0.0026\n",
            "Epoch 10, Batch 44000, Loss: 0.1029\n",
            "Epoch 10, Batch 44500, Loss: 0.0155\n",
            "Epoch 10, Batch 45000, Loss: 0.1135\n",
            "Epoch 10, Batch 45500, Loss: 0.0017\n",
            "Epoch 10, Batch 46000, Loss: 0.0559\n",
            "Epoch 10, Batch 46500, Loss: 0.0059\n",
            "Epoch 10, Batch 47000, Loss: 0.0095\n",
            "Epoch 10, Batch 47500, Loss: 0.0105\n",
            "Epoch 10, Batch 48000, Loss: 0.0295\n",
            "Epoch 10, Batch 48500, Loss: 0.0095\n",
            "Epoch 10, Batch 49000, Loss: 0.0232\n",
            "Epoch 10, Batch 49500, Loss: 0.0379\n",
            "Epoch 10, Batch 50000, Loss: 0.0120\n",
            "Epoch 10, Batch 50500, Loss: 0.0088\n",
            "Epoch 10, Batch 51000, Loss: 0.1473\n",
            "Epoch 10, Batch 51500, Loss: 0.0325\n",
            "Epoch 10, Batch 52000, Loss: 0.4978\n",
            "Epoch 10, Batch 52500, Loss: 0.0013\n",
            "Epoch 10, Batch 53000, Loss: 0.0376\n",
            "Epoch 10, Batch 53500, Loss: 0.0349\n",
            "Epoch 10, Batch 54000, Loss: 0.0039\n",
            "Epoch 10, Batch 54500, Loss: 0.0215\n",
            "Epoch 10, Batch 55000, Loss: 0.0175\n",
            "Epoch 10, Batch 55500, Loss: 0.0117\n",
            "Epoch 10, Batch 56000, Loss: 0.1254\n",
            "Epoch 10, Batch 56500, Loss: 0.0379\n",
            "Epoch 10, Batch 57000, Loss: 0.0046\n",
            "Epoch 10, Batch 57500, Loss: 0.0265\n",
            "Epoch 10, Batch 58000, Loss: 0.0090\n",
            "Epoch 10, Batch 58500, Loss: 0.0250\n",
            "Epoch 10, Batch 59000, Loss: 0.0035\n",
            "Epoch 10, Batch 59500, Loss: 0.0334\n",
            "Epoch 10, Batch 60000, Loss: 0.0038\n",
            "Epoch 10, Batch 60500, Loss: 0.0017\n",
            "Epoch 10, Batch 61000, Loss: 0.0018\n",
            "Epoch 10, Batch 61500, Loss: 0.0115\n",
            "Epoch 10, Batch 62000, Loss: 0.0383\n",
            "Epoch 10, Batch 62500, Loss: 0.0156\n",
            "Epoch 10, Batch 63000, Loss: 0.0076\n",
            "Epoch 10, Batch 63500, Loss: 0.0165\n",
            "Epoch 10, Batch 64000, Loss: 0.0401\n",
            "Epoch 10, Batch 64500, Loss: 0.0300\n",
            "Epoch 10, Batch 65000, Loss: 0.0149\n",
            "Epoch 10, Batch 65500, Loss: 0.0098\n",
            "Epoch 10, Batch 66000, Loss: 0.0844\n",
            "Epoch 10, Batch 66500, Loss: 0.0232\n",
            "Epoch 10, Batch 67000, Loss: 0.0575\n",
            "Epoch 10, Batch 67500, Loss: 0.0046\n",
            "Epoch 10, Batch 68000, Loss: 0.0454\n",
            "Epoch 10, Batch 68500, Loss: 0.0033\n",
            "Epoch 10, Batch 69000, Loss: 0.0071\n",
            "Epoch 10, Batch 69500, Loss: 0.0027\n",
            "Epoch 10, Batch 70000, Loss: 0.0733\n",
            "Epoch 10, Batch 70500, Loss: 0.0286\n",
            "Epoch 10, Batch 71000, Loss: 0.0118\n",
            "Epoch 10, Batch 71500, Loss: 0.0398\n",
            "Epoch 10, Batch 72000, Loss: 0.0133\n",
            "Epoch 10, Batch 72500, Loss: 0.0207\n",
            "Epoch 10, Batch 73000, Loss: 0.0498\n",
            "Epoch 10, Batch 73500, Loss: 0.0020\n",
            "Epoch 10, Average Loss: 0.0452\n",
            "Test Loss: 0.1315\n",
            "æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: 28512 è¡Œ\n",
            "ç©ºå€¼æ•°é‡: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#å°è¯•èåˆå¤–éƒ¨æ•°æ®ï¼Œæ•ˆæœä¸ä½³éœ€è¦å°è¯•åˆ«çš„æ¨¡å‹\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def load_and_merge_data():\n",
        "    \"\"\"ä¸€æ¬¡æ€§åŠ è½½å¹¶èåˆæ‰€æœ‰æ•°æ®\"\"\"\n",
        "    print(\"åŠ è½½æ•°æ®...\")\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "\n",
        "    # ç»Ÿä¸€çš„èåˆå‡½æ•°\n",
        "    def merge_external(df, is_train=True):\n",
        "        # èåˆtransactions\n",
        "        try:\n",
        "            trans_df = pd.read_csv('transactions.csv')\n",
        "            trans_df['date'] = pd.to_datetime(trans_df['date'])\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.merge(trans_df, on=['date', 'store_nbr'], how='left')\n",
        "            df['transactions'] = df['transactions'].fillna(0)\n",
        "            if is_train:\n",
        "                df['avg_basket_size'] = df['sales'] / (df['transactions'] + 1)\n",
        "            else:\n",
        "                df['avg_basket_size'] = 0\n",
        "            print(\"âœ“ transactions.csv èåˆæˆåŠŸ\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"âœ— transactions.csv æœªæ‰¾åˆ°\")\n",
        "            df['transactions'] = df['avg_basket_size'] = 0\n",
        "\n",
        "        # èåˆstores\n",
        "        try:\n",
        "            stores_df = pd.read_csv('stores.csv')\n",
        "            df = df.merge(stores_df, on='store_nbr', how='left')\n",
        "\n",
        "            # æ‰¹é‡ç¼–ç åˆ†ç±»å˜é‡\n",
        "            for col in ['city', 'state', 'type']:\n",
        "                if col in df.columns:\n",
        "                    le = LabelEncoder()\n",
        "                    df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str).fillna('unknown'))\n",
        "            df['cluster'] = df['cluster'].fillna(0)\n",
        "            print(\"âœ“ stores.csv èåˆæˆåŠŸ\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"âœ— stores.csv æœªæ‰¾åˆ°\")\n",
        "            for col in ['city_encoded', 'state_encoded', 'type_encoded', 'cluster']:\n",
        "                df[col] = 0\n",
        "\n",
        "        return df\n",
        "\n",
        "    train_df = merge_external(train_df, True)\n",
        "    test_df = merge_external(test_df, False)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def create_features(df, is_train=True):\n",
        "    \"\"\"æ•°æ®ç‰¹å¾å·¥ç¨‹\"\"\"\n",
        "    print(\"ç‰¹å¾å·¥ç¨‹ä¸­...\")\n",
        "    df = df.copy()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    if is_train:\n",
        "        df = df.dropna(subset=['sales'])\n",
        "        df = df[df['sales'] >= 0]\n",
        "\n",
        "    # æ‰¹é‡åˆ›å»ºæ—¶é—´ç‰¹å¾\n",
        "    time_features = {\n",
        "        'month': df['date'].dt.month,\n",
        "        'dayofweek': df['date'].dt.dayofweek,\n",
        "        'day': df['date'].dt.day,\n",
        "        'quarter': df['date'].dt.quarter,\n",
        "        'is_weekend': (df['date'].dt.dayofweek >= 5).astype(int),\n",
        "        'is_month_start': (df['date'].dt.day <= 5).astype(int),\n",
        "        'is_month_end': (df['date'].dt.day >= 25).astype(int)\n",
        "    }\n",
        "    for name, values in time_features.items():\n",
        "        df[name] = values\n",
        "\n",
        "    df['onpromotion'] = df['onpromotion'].fillna(0)\n",
        "    df = df.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
        "\n",
        "    # é”€å”®ç›¸å…³ç‰¹å¾\n",
        "    if is_train:\n",
        "        # æ»åç‰¹å¾\n",
        "        for lag in [1, 7, 14, 28]:\n",
        "            df[f'sales_lag{lag}'] = df.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
        "\n",
        "        # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾\n",
        "        for window in [7, 14, 28]:\n",
        "            rolling = df.groupby(['store_nbr', 'family'])['sales'].rolling(window, min_periods=1)\n",
        "            df[f'sales_rolling_mean_{window}'] = rolling.mean().values\n",
        "            df[f'sales_rolling_std_{window}'] = rolling.std().values\n",
        "\n",
        "    # ä¿ƒé”€ç‰¹å¾\n",
        "    df['promo_lag1'] = df.groupby(['store_nbr', 'family'])['onpromotion'].shift(1)\n",
        "    df['promo_rolling_sum_7'] = df.groupby(['store_nbr', 'family'])['onpromotion'].rolling(7, min_periods=1).sum().values\n",
        "\n",
        "    # äº¤æ˜“ç‰¹å¾\n",
        "    if 'transactions' in df.columns and df['transactions'].sum() > 0:\n",
        "        df['transactions_lag7'] = df.groupby(['store_nbr', 'family'])['transactions'].shift(7)\n",
        "\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    feature_count = len([col for col in df.columns if col not in ['id', 'date', 'store_nbr', 'family', 'sales']])\n",
        "    print(f\"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: {feature_count}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_sequences(df, seq_len=14, pred_len=15):\n",
        "    \"\"\"æ—¶åºå¤„ç† - åˆ›å»ºæ—¶é—´åºåˆ—\"\"\"\n",
        "    print(\"æ—¶åºå¤„ç†ä¸­...\")\n",
        "\n",
        "    feature_cols = [\n",
        "        'month', 'dayofweek', 'day', 'quarter', 'is_weekend', 'is_month_start', 'is_month_end',\n",
        "        'onpromotion', 'sales_lag1', 'sales_lag7', 'sales_lag14', 'sales_lag28',\n",
        "        'sales_rolling_mean_7', 'sales_rolling_mean_14', 'sales_rolling_mean_28',\n",
        "        'sales_rolling_std_7', 'sales_rolling_std_14', 'sales_rolling_std_28',\n",
        "        'promo_lag1', 'promo_rolling_sum_7', 'transactions', 'avg_basket_size',\n",
        "        'transactions_lag7', 'city_encoded', 'state_encoded', 'type_encoded', 'cluster'\n",
        "    ]\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for (store, family), group in df.groupby(['store_nbr', 'family']):\n",
        "        group = group.sort_values('date')\n",
        "        if len(group) < seq_len + pred_len:\n",
        "            continue\n",
        "\n",
        "        features = group[feature_cols].values\n",
        "        sales = group['sales'].values\n",
        "\n",
        "        for i in range(len(group) - seq_len - pred_len + 1):\n",
        "            X.append(features[i:i+seq_len])\n",
        "            y.append(sales[i+seq_len:i+seq_len+pred_len])\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    print(f\"æ—¶åºå¤„ç†å®Œæˆï¼Œåºåˆ—å½¢çŠ¶: X{X.shape}, y{y.shape}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def create_dataloaders(X, y, batch_size=32):\n",
        "    \"\"\"æ‰¹é‡åŒ–å¤„ç† - åˆ›å»ºæ•°æ®åŠ è½½å™¨\"\"\"\n",
        "    print(\"æ‰¹é‡åŒ–å¤„ç†ä¸­...\")\n",
        "\n",
        "    # ç‰¹å¾æ ‡å‡†åŒ–\n",
        "    scaler_X = StandardScaler()\n",
        "    n_samples, seq_len, n_features = X.shape\n",
        "    X_scaled = scaler_X.fit_transform(X.reshape(-1, n_features))\n",
        "    X_scaled = X_scaled.reshape(n_samples, seq_len, n_features)\n",
        "\n",
        "    # ç›®æ ‡å˜é‡æ ‡å‡†åŒ–\n",
        "    scaler_y = StandardScaler()\n",
        "    y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "    # è®­ç»ƒéªŒè¯åˆ†å‰²\n",
        "    split = int(0.8 * len(X_scaled))\n",
        "    X_train, X_val = X_scaled[:split], X_scaled[split:]\n",
        "    y_train, y_val = y_scaled[:split], y_scaled[split:]\n",
        "\n",
        "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    print(f\"æ‰¹é‡åŒ–å¤„ç†å®Œæˆï¼Œè®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}, éªŒè¯æ‰¹æ¬¡: {len(val_loader)}\")\n",
        "\n",
        "    return train_loader, val_loader, scaler_X, scaler_y\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    \"\"\"æ¨¡å‹æ„å»º - LSTMæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹\"\"\"\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=15):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "        if x.is_cuda:\n",
        "            h_0 = h_0.cuda()\n",
        "            c_0 = c_0.cuda()\n",
        "\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        output = self.fc(last_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def train_and_evaluate(train_loader, val_loader, input_size, device, num_epochs=10):\n",
        "    \"\"\"è®­ç»ƒè¯„ä¼°æ¨¡å‹\"\"\"\n",
        "    print(\"å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
        "\n",
        "    # åˆ›å»ºæ¨¡å‹\n",
        "    model = LSTM(input_size=input_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # è®­ç»ƒå¾ªç¯\n",
        "    for epoch in range(num_epochs):\n",
        "        # è®­ç»ƒé˜¶æ®µ\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            # å‰å‘ä¼ æ’­\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # åå‘ä¼ æ’­\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 500 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # éªŒè¯é˜¶æ®µ\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    print(\"æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
        "    return model\n",
        "\n",
        "def prepare_test_data(test_df, train_df, scaler_X, scaler_y):\n",
        "    \"\"\"å‡†å¤‡æµ‹è¯•æ•°æ®\"\"\"\n",
        "    print(\"å‡†å¤‡æµ‹è¯•æ•°æ®...\")\n",
        "\n",
        "    test_df = test_df.copy()\n",
        "    test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "    test_df['onpromotion'] = test_df['onpromotion'].fillna(0)\n",
        "\n",
        "    # æ—¶é—´ç‰¹å¾\n",
        "    time_features = {\n",
        "        'month': test_df['date'].dt.month,\n",
        "        'dayofweek': test_df['date'].dt.dayofweek,\n",
        "        'day': test_df['date'].dt.day,\n",
        "        'quarter': test_df['date'].dt.quarter,\n",
        "        'is_weekend': (test_df['date'].dt.dayofweek >= 5).astype(int),\n",
        "        'is_month_start': (test_df['date'].dt.day <= 5).astype(int),\n",
        "        'is_month_end': (test_df['date'].dt.day >= 25).astype(int)\n",
        "    }\n",
        "    for name, values in time_features.items():\n",
        "        test_df[name] = values\n",
        "\n",
        "    # ä»è®­ç»ƒæ•°æ®è·å–ç»Ÿè®¡ä¿¡æ¯\n",
        "    train_stats = train_df.groupby(['store_nbr', 'family'])['sales'].agg(['last', 'mean', 'std']).reset_index()\n",
        "    train_stats.columns = ['store_nbr', 'family', 'sales_last', 'sales_mean', 'sales_std']\n",
        "    test_df = test_df.merge(train_stats, on=['store_nbr', 'family'], how='left')\n",
        "\n",
        "    # å¡«å……é”€å”®ç‰¹å¾\n",
        "    for lag in [1, 7, 14, 28]:\n",
        "        test_df[f'sales_lag{lag}'] = test_df['sales_last']\n",
        "\n",
        "    for window in [7, 14, 28]:\n",
        "        test_df[f'sales_rolling_mean_{window}'] = test_df['sales_mean']\n",
        "        test_df[f'sales_rolling_std_{window}'] = test_df['sales_std']\n",
        "\n",
        "    # ä¿ƒé”€å’Œå¤–éƒ¨ç‰¹å¾\n",
        "    test_df['promo_lag1'] = test_df['onpromotion']\n",
        "    test_df['promo_rolling_sum_7'] = test_df['onpromotion'] * 3\n",
        "\n",
        "    for col in ['transactions', 'avg_basket_size', 'transactions_lag7', 'city_encoded', 'state_encoded', 'type_encoded', 'cluster']:\n",
        "        if col not in test_df.columns:\n",
        "            test_df[col] = 0\n",
        "\n",
        "    test_df = test_df.fillna(0)\n",
        "\n",
        "    # ç‰¹å¾åˆ—è¡¨\n",
        "    feature_cols = [\n",
        "        'month', 'dayofweek', 'day', 'quarter', 'is_weekend', 'is_month_start', 'is_month_end',\n",
        "        'onpromotion', 'sales_lag1', 'sales_lag7', 'sales_lag14', 'sales_lag28',\n",
        "        'sales_rolling_mean_7', 'sales_rolling_mean_14', 'sales_rolling_mean_28',\n",
        "        'sales_rolling_std_7', 'sales_rolling_std_14', 'sales_rolling_std_28',\n",
        "        'promo_lag1', 'promo_rolling_sum_7', 'transactions', 'avg_basket_size',\n",
        "        'transactions_lag7', 'city_encoded', 'state_encoded', 'type_encoded', 'cluster'\n",
        "    ]\n",
        "\n",
        "    X_test = []\n",
        "    predictions_info = []\n",
        "\n",
        "    for (store, family), group in test_df.groupby(['store_nbr', 'family']):\n",
        "        group = group.sort_values('date')\n",
        "        if len(group) >= 14:\n",
        "            X_test.append(group[feature_cols].iloc[:14].values)\n",
        "        else:\n",
        "            X_test.append(np.zeros((14, len(feature_cols))))\n",
        "\n",
        "        predictions_info.append({\n",
        "            'store': store,\n",
        "            'family': family,\n",
        "            'ids': group['id'].values\n",
        "        })\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    n_samples, seq_len, n_features = X_test.shape\n",
        "    X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features))\n",
        "    X_test_scaled = X_test_scaled.reshape(n_samples, seq_len, n_features)\n",
        "\n",
        "    return X_test_scaled, predictions_info\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 1. æ•°æ®åŠ è½½å’Œèåˆ\n",
        "    train_data, test_data = load_and_merge_data()\n",
        "\n",
        "    # 2. æ•°æ®ç‰¹å¾å·¥ç¨‹\n",
        "    train_featured = create_features(train_data, is_train=True)\n",
        "\n",
        "    # 3. æ—¶åºå¤„ç†\n",
        "    X, y = create_sequences(train_featured)\n",
        "\n",
        "    # 4. æ‰¹é‡åŒ–å¤„ç†\n",
        "    train_loader, val_loader, scaler_X, scaler_y = create_dataloaders(X, y)\n",
        "\n",
        "    # 5. è®­ç»ƒè¯„ä¼°æ¨¡å‹\n",
        "    model = train_and_evaluate(train_loader, val_loader, X.shape[2], device)\n",
        "\n",
        "    # 6. æµ‹è¯•æ•°æ®å¤„ç†å’Œé¢„æµ‹\n",
        "    test_featured = create_features(test_data, is_train=False)\n",
        "    X_test, predictions_info = prepare_test_data(test_featured, train_featured, scaler_X, scaler_y)\n",
        "\n",
        "    # 7. ç”Ÿæˆé¢„æµ‹\n",
        "    print(\"ç”Ÿæˆé¢„æµ‹...\")\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions_scaled = model(X_test_tensor)\n",
        "\n",
        "    predictions = scaler_y.inverse_transform(predictions_scaled.cpu().numpy())\n",
        "\n",
        "    # 8. ç”Ÿæˆæäº¤æ–‡ä»¶\n",
        "    submission_data = {}\n",
        "    for i, info in enumerate(predictions_info):\n",
        "        pred_15days = predictions[i]\n",
        "        for j, test_id in enumerate(info['ids']):\n",
        "            if j < len(pred_15days):\n",
        "                submission_data[test_id] = max(0, pred_15days[j])\n",
        "            else:\n",
        "                submission_data[test_id] = max(0, pred_15days[-1])\n",
        "\n",
        "    submission_df = pd.read_csv('test.csv')[['id']].copy()\n",
        "    submission_df['sales'] = submission_df['id'].map(submission_data).fillna(0)\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "    print(f\"å®Œæˆï¼ç”Ÿæˆ {len(submission_df)} è¡Œé¢„æµ‹ç»“æœ\")\n",
        "    print(f\"é¢„æµ‹ç»Ÿè®¡: {submission_df['sales'].describe()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuEznkYcvKJn",
        "outputId": "ca49fffb-ae03-45da-8921-6f4d71c8a0da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "åŠ è½½æ•°æ®...\n",
            "âœ“ transactions.csv èåˆæˆåŠŸ\n",
            "âœ“ stores.csv èåˆæˆåŠŸ\n",
            "âœ“ transactions.csv èåˆæˆåŠŸ\n",
            "âœ“ stores.csv èåˆæˆåŠŸ\n",
            "ğŸ¯ ç‰¹å¾å·¥ç¨‹ä¸­...\n",
            "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: 30\n",
            "â° æ—¶åºå¤„ç†ä¸­...\n",
            "âœ… æ—¶åºå¤„ç†å®Œæˆï¼Œåºåˆ—å½¢çŠ¶: X(2950992, 14, 27), y(2950992, 15)\n",
            "ğŸ”„ æ‰¹é‡åŒ–å¤„ç†ä¸­...\n",
            "âœ… æ‰¹é‡åŒ–å¤„ç†å®Œæˆï¼Œè®­ç»ƒæ‰¹æ¬¡: 73775, éªŒè¯æ‰¹æ¬¡: 18444\n",
            "ğŸ”¥ å¼€å§‹è®­ç»ƒæ¨¡å‹...\n",
            "Epoch 1, Batch 500, Loss: 0.0243\n",
            "Epoch 1, Batch 1000, Loss: 0.0489\n",
            "Epoch 1, Batch 1500, Loss: 0.0317\n",
            "Epoch 1, Batch 2000, Loss: 0.0821\n",
            "Epoch 1, Batch 2500, Loss: 0.0959\n",
            "Epoch 1, Batch 3000, Loss: 0.0243\n",
            "Epoch 1, Batch 3500, Loss: 0.1579\n",
            "Epoch 1, Batch 4000, Loss: 0.0285\n",
            "Epoch 1, Batch 4500, Loss: 0.0516\n",
            "Epoch 1, Batch 5000, Loss: 0.0256\n",
            "Epoch 1, Batch 5500, Loss: 0.1020\n",
            "Epoch 1, Batch 6000, Loss: 0.0273\n",
            "Epoch 1, Batch 6500, Loss: 0.0351\n",
            "Epoch 1, Batch 7000, Loss: 0.0129\n",
            "Epoch 1, Batch 7500, Loss: 0.0070\n",
            "Epoch 1, Batch 8000, Loss: 0.1554\n",
            "Epoch 1, Batch 8500, Loss: 0.0453\n",
            "Epoch 1, Batch 9000, Loss: 0.0583\n",
            "Epoch 1, Batch 9500, Loss: 0.2252\n",
            "Epoch 1, Batch 10000, Loss: 0.0559\n",
            "Epoch 1, Batch 10500, Loss: 0.0514\n",
            "Epoch 1, Batch 11000, Loss: 0.0449\n",
            "Epoch 1, Batch 11500, Loss: 0.0027\n",
            "Epoch 1, Batch 12000, Loss: 0.0138\n",
            "Epoch 1, Batch 12500, Loss: 0.0344\n",
            "Epoch 1, Batch 13000, Loss: 0.0411\n",
            "Epoch 1, Batch 13500, Loss: 0.1100\n",
            "Epoch 1, Batch 14000, Loss: 0.0502\n",
            "Epoch 1, Batch 14500, Loss: 0.0287\n",
            "Epoch 1, Batch 15000, Loss: 0.0552\n",
            "Epoch 1, Batch 15500, Loss: 0.0280\n",
            "Epoch 1, Batch 16000, Loss: 0.0587\n",
            "Epoch 1, Batch 16500, Loss: 0.0763\n",
            "Epoch 1, Batch 17000, Loss: 0.0082\n",
            "Epoch 1, Batch 17500, Loss: 0.0036\n",
            "Epoch 1, Batch 18000, Loss: 0.0696\n",
            "Epoch 1, Batch 18500, Loss: 0.0863\n",
            "Epoch 1, Batch 19000, Loss: 0.1037\n",
            "Epoch 1, Batch 19500, Loss: 0.3938\n",
            "Epoch 1, Batch 20000, Loss: 0.0047\n",
            "Epoch 1, Batch 20500, Loss: 0.0098\n",
            "Epoch 1, Batch 21000, Loss: 0.0110\n",
            "Epoch 1, Batch 21500, Loss: 0.0348\n",
            "Epoch 1, Batch 22000, Loss: 0.3996\n",
            "Epoch 1, Batch 22500, Loss: 0.0694\n",
            "Epoch 1, Batch 23000, Loss: 0.0229\n",
            "Epoch 1, Batch 23500, Loss: 0.0301\n",
            "Epoch 1, Batch 24000, Loss: 0.0394\n",
            "Epoch 1, Batch 24500, Loss: 0.0016\n",
            "Epoch 1, Batch 25000, Loss: 0.0065\n",
            "Epoch 1, Batch 25500, Loss: 0.0998\n",
            "Epoch 1, Batch 26000, Loss: 1.2134\n",
            "Epoch 1, Batch 26500, Loss: 0.0194\n",
            "Epoch 1, Batch 27000, Loss: 0.0171\n",
            "Epoch 1, Batch 27500, Loss: 0.0087\n",
            "Epoch 1, Batch 28000, Loss: 0.0149\n",
            "Epoch 1, Batch 28500, Loss: 0.0019\n",
            "Epoch 1, Batch 29000, Loss: 0.1215\n",
            "Epoch 1, Batch 29500, Loss: 0.0214\n",
            "Epoch 1, Batch 30000, Loss: 0.6744\n",
            "Epoch 1, Batch 30500, Loss: 0.1028\n",
            "Epoch 1, Batch 31000, Loss: 0.0366\n",
            "Epoch 1, Batch 31500, Loss: 0.2186\n",
            "Epoch 1, Batch 32000, Loss: 0.0224\n",
            "Epoch 1, Batch 32500, Loss: 0.0201\n",
            "Epoch 1, Batch 33000, Loss: 0.0083\n",
            "Epoch 1, Batch 33500, Loss: 0.0362\n",
            "Epoch 1, Batch 34000, Loss: 0.0069\n",
            "Epoch 1, Batch 34500, Loss: 0.0272\n",
            "Epoch 1, Batch 35000, Loss: 0.1426\n",
            "Epoch 1, Batch 35500, Loss: 0.0126\n",
            "Epoch 1, Batch 36000, Loss: 0.0092\n",
            "Epoch 1, Batch 36500, Loss: 0.0384\n",
            "Epoch 1, Batch 37000, Loss: 0.0129\n",
            "Epoch 1, Batch 37500, Loss: 0.0230\n",
            "Epoch 1, Batch 38000, Loss: 0.0347\n",
            "Epoch 1, Batch 38500, Loss: 0.1704\n",
            "Epoch 1, Batch 39000, Loss: 0.0446\n",
            "Epoch 1, Batch 39500, Loss: 0.0714\n",
            "Epoch 1, Batch 40000, Loss: 7.0427\n",
            "Epoch 1, Batch 40500, Loss: 0.0227\n",
            "Epoch 1, Batch 41000, Loss: 0.0373\n",
            "Epoch 1, Batch 41500, Loss: 0.0070\n",
            "Epoch 1, Batch 42000, Loss: 0.1101\n",
            "Epoch 1, Batch 42500, Loss: 0.0285\n",
            "Epoch 1, Batch 43000, Loss: 0.0168\n",
            "Epoch 1, Batch 43500, Loss: 0.0180\n",
            "Epoch 1, Batch 44000, Loss: 0.0144\n",
            "Epoch 1, Batch 44500, Loss: 0.0255\n",
            "Epoch 1, Batch 45000, Loss: 0.0280\n",
            "Epoch 1, Batch 45500, Loss: 0.0739\n",
            "Epoch 1, Batch 46000, Loss: 0.0012\n",
            "Epoch 1, Batch 46500, Loss: 0.0256\n",
            "Epoch 1, Batch 47000, Loss: 0.0297\n",
            "Epoch 1, Batch 47500, Loss: 0.0305\n",
            "Epoch 1, Batch 48000, Loss: 0.0105\n",
            "Epoch 1, Batch 48500, Loss: 0.0070\n",
            "Epoch 1, Batch 49000, Loss: 0.0065\n",
            "Epoch 1, Batch 49500, Loss: 0.0173\n",
            "Epoch 1, Batch 50000, Loss: 0.0020\n",
            "Epoch 1, Batch 50500, Loss: 0.0557\n",
            "Epoch 1, Batch 51000, Loss: 0.0567\n",
            "Epoch 1, Batch 51500, Loss: 0.0326\n",
            "Epoch 1, Batch 52000, Loss: 0.1245\n",
            "Epoch 1, Batch 52500, Loss: 0.1898\n",
            "Epoch 1, Batch 53000, Loss: 0.0138\n",
            "Epoch 1, Batch 53500, Loss: 0.0567\n",
            "Epoch 1, Batch 54000, Loss: 0.0066\n",
            "Epoch 1, Batch 54500, Loss: 0.0121\n",
            "Epoch 1, Batch 55000, Loss: 0.1534\n",
            "Epoch 1, Batch 55500, Loss: 0.0182\n",
            "Epoch 1, Batch 56000, Loss: 0.0091\n",
            "Epoch 1, Batch 56500, Loss: 0.1211\n",
            "Epoch 1, Batch 57000, Loss: 0.0658\n",
            "Epoch 1, Batch 57500, Loss: 0.0194\n",
            "Epoch 1, Batch 58000, Loss: 0.0200\n",
            "Epoch 1, Batch 58500, Loss: 0.0904\n",
            "Epoch 1, Batch 59000, Loss: 0.0062\n",
            "Epoch 1, Batch 59500, Loss: 0.0065\n",
            "Epoch 1, Batch 60000, Loss: 0.0338\n",
            "Epoch 1, Batch 60500, Loss: 0.0225\n",
            "Epoch 1, Batch 61000, Loss: 0.0941\n",
            "Epoch 1, Batch 61500, Loss: 0.0231\n",
            "Epoch 1, Batch 62000, Loss: 0.1009\n",
            "Epoch 1, Batch 62500, Loss: 0.0111\n",
            "Epoch 1, Batch 63000, Loss: 0.0139\n",
            "Epoch 1, Batch 63500, Loss: 0.0266\n",
            "Epoch 1, Batch 64000, Loss: 0.1881\n",
            "Epoch 1, Batch 64500, Loss: 0.5062\n",
            "Epoch 1, Batch 65000, Loss: 0.0266\n",
            "Epoch 1, Batch 65500, Loss: 0.0016\n",
            "Epoch 1, Batch 66000, Loss: 0.0518\n",
            "Epoch 1, Batch 66500, Loss: 0.0233\n",
            "Epoch 1, Batch 67000, Loss: 0.0049\n",
            "Epoch 1, Batch 67500, Loss: 0.2895\n",
            "Epoch 1, Batch 68000, Loss: 0.0350\n",
            "Epoch 1, Batch 68500, Loss: 0.0046\n",
            "Epoch 1, Batch 69000, Loss: 0.0147\n",
            "Epoch 1, Batch 69500, Loss: 0.0636\n",
            "Epoch 1, Batch 70000, Loss: 0.0367\n",
            "Epoch 1, Batch 70500, Loss: 0.0496\n",
            "Epoch 1, Batch 71000, Loss: 0.0432\n",
            "Epoch 1, Batch 71500, Loss: 0.0059\n",
            "Epoch 1, Batch 72000, Loss: 0.0006\n",
            "Epoch 1, Batch 72500, Loss: 0.0028\n",
            "Epoch 1, Batch 73000, Loss: 0.0113\n",
            "Epoch 1, Batch 73500, Loss: 0.0222\n",
            "Epoch 1, Train Loss: 0.0782, Val Loss: 0.2225\n",
            "Epoch 2, Batch 500, Loss: 0.0259\n",
            "Epoch 2, Batch 1000, Loss: 0.1588\n",
            "Epoch 2, Batch 1500, Loss: 0.0255\n",
            "Epoch 2, Batch 2000, Loss: 0.0018\n",
            "Epoch 2, Batch 2500, Loss: 0.0512\n",
            "Epoch 2, Batch 3000, Loss: 0.0275\n",
            "Epoch 2, Batch 3500, Loss: 0.0305\n",
            "Epoch 2, Batch 4000, Loss: 0.0066\n",
            "Epoch 2, Batch 4500, Loss: 0.0789\n",
            "Epoch 2, Batch 5000, Loss: 0.0139\n",
            "Epoch 2, Batch 5500, Loss: 0.0346\n",
            "Epoch 2, Batch 6000, Loss: 0.0292\n",
            "Epoch 2, Batch 6500, Loss: 0.1446\n",
            "Epoch 2, Batch 7000, Loss: 0.0268\n",
            "Epoch 2, Batch 7500, Loss: 0.1034\n",
            "Epoch 2, Batch 8000, Loss: 0.0021\n",
            "Epoch 2, Batch 8500, Loss: 0.0021\n",
            "Epoch 2, Batch 9000, Loss: 0.0073\n",
            "Epoch 2, Batch 9500, Loss: 0.0042\n",
            "Epoch 2, Batch 10000, Loss: 0.0370\n",
            "Epoch 2, Batch 10500, Loss: 0.2072\n",
            "Epoch 2, Batch 11000, Loss: 0.0197\n",
            "Epoch 2, Batch 11500, Loss: 0.1923\n",
            "Epoch 2, Batch 12000, Loss: 0.0096\n",
            "Epoch 2, Batch 12500, Loss: 0.0080\n",
            "Epoch 2, Batch 13000, Loss: 0.0507\n",
            "Epoch 2, Batch 13500, Loss: 0.0208\n",
            "Epoch 2, Batch 14000, Loss: 0.0018\n",
            "Epoch 2, Batch 14500, Loss: 0.0235\n",
            "Epoch 2, Batch 15000, Loss: 0.0112\n",
            "Epoch 2, Batch 15500, Loss: 0.0713\n",
            "Epoch 2, Batch 16000, Loss: 0.0232\n",
            "Epoch 2, Batch 16500, Loss: 0.1029\n",
            "Epoch 2, Batch 17000, Loss: 0.0177\n",
            "Epoch 2, Batch 17500, Loss: 0.0711\n",
            "Epoch 2, Batch 18000, Loss: 0.0651\n",
            "Epoch 2, Batch 18500, Loss: 0.0257\n",
            "Epoch 2, Batch 19000, Loss: 0.0690\n",
            "Epoch 2, Batch 19500, Loss: 0.0029\n",
            "Epoch 2, Batch 20000, Loss: 0.0685\n",
            "Epoch 2, Batch 20500, Loss: 0.0386\n",
            "Epoch 2, Batch 21000, Loss: 0.0034\n",
            "Epoch 2, Batch 21500, Loss: 0.0129\n",
            "Epoch 2, Batch 22000, Loss: 0.0030\n",
            "Epoch 2, Batch 22500, Loss: 0.0110\n",
            "Epoch 2, Batch 23000, Loss: 0.0257\n",
            "Epoch 2, Batch 23500, Loss: 0.0174\n",
            "Epoch 2, Batch 24000, Loss: 0.0721\n",
            "Epoch 2, Batch 24500, Loss: 0.0080\n",
            "Epoch 2, Batch 25000, Loss: 0.0154\n",
            "Epoch 2, Batch 25500, Loss: 0.0066\n",
            "Epoch 2, Batch 26000, Loss: 0.0516\n",
            "Epoch 2, Batch 26500, Loss: 0.0130\n",
            "Epoch 2, Batch 27000, Loss: 0.0473\n",
            "Epoch 2, Batch 27500, Loss: 0.0076\n",
            "Epoch 2, Batch 28000, Loss: 0.0222\n",
            "Epoch 2, Batch 28500, Loss: 0.0164\n",
            "Epoch 2, Batch 29000, Loss: 0.0332\n",
            "Epoch 2, Batch 29500, Loss: 0.0127\n",
            "Epoch 2, Batch 30000, Loss: 0.0107\n",
            "Epoch 2, Batch 30500, Loss: 0.1413\n",
            "Epoch 2, Batch 31000, Loss: 0.0603\n",
            "Epoch 2, Batch 31500, Loss: 0.0040\n",
            "Epoch 2, Batch 32000, Loss: 0.0052\n",
            "Epoch 2, Batch 32500, Loss: 0.0205\n",
            "Epoch 2, Batch 33000, Loss: 0.0160\n",
            "Epoch 2, Batch 33500, Loss: 0.0471\n",
            "Epoch 2, Batch 34000, Loss: 0.0202\n",
            "Epoch 2, Batch 34500, Loss: 0.0258\n",
            "Epoch 2, Batch 35000, Loss: 0.0433\n",
            "Epoch 2, Batch 35500, Loss: 0.0025\n",
            "Epoch 2, Batch 36000, Loss: 0.0776\n",
            "Epoch 2, Batch 36500, Loss: 0.0549\n",
            "Epoch 2, Batch 37000, Loss: 0.0636\n",
            "Epoch 2, Batch 37500, Loss: 0.0611\n",
            "Epoch 2, Batch 38000, Loss: 0.8734\n",
            "Epoch 2, Batch 38500, Loss: 0.0562\n",
            "Epoch 2, Batch 39000, Loss: 0.0045\n",
            "Epoch 2, Batch 39500, Loss: 0.0066\n",
            "Epoch 2, Batch 40000, Loss: 0.0203\n",
            "Epoch 2, Batch 40500, Loss: 0.0228\n",
            "Epoch 2, Batch 41000, Loss: 0.0366\n",
            "Epoch 2, Batch 41500, Loss: 0.0078\n",
            "Epoch 2, Batch 42000, Loss: 0.0919\n",
            "Epoch 2, Batch 42500, Loss: 0.0086\n",
            "Epoch 2, Batch 43000, Loss: 0.0067\n",
            "Epoch 2, Batch 43500, Loss: 0.0521\n",
            "Epoch 2, Batch 44000, Loss: 0.0772\n",
            "Epoch 2, Batch 44500, Loss: 0.0669\n",
            "Epoch 2, Batch 45000, Loss: 0.0357\n",
            "Epoch 2, Batch 45500, Loss: 0.0028\n",
            "Epoch 2, Batch 46000, Loss: 0.0412\n",
            "Epoch 2, Batch 46500, Loss: 0.0256\n",
            "Epoch 2, Batch 47000, Loss: 0.0211\n",
            "Epoch 2, Batch 47500, Loss: 0.0460\n",
            "Epoch 2, Batch 48000, Loss: 0.0787\n",
            "Epoch 2, Batch 48500, Loss: 0.1212\n",
            "Epoch 2, Batch 49000, Loss: 0.0034\n",
            "Epoch 2, Batch 49500, Loss: 0.1445\n",
            "Epoch 2, Batch 50000, Loss: 0.1231\n",
            "Epoch 2, Batch 50500, Loss: 0.0303\n",
            "Epoch 2, Batch 51000, Loss: 0.0063\n",
            "Epoch 2, Batch 51500, Loss: 0.0708\n",
            "Epoch 2, Batch 52000, Loss: 0.0163\n",
            "Epoch 2, Batch 52500, Loss: 0.0075\n",
            "Epoch 2, Batch 53000, Loss: 0.0077\n",
            "Epoch 2, Batch 53500, Loss: 0.0031\n",
            "Epoch 2, Batch 54000, Loss: 0.0025\n",
            "Epoch 2, Batch 54500, Loss: 0.0042\n",
            "Epoch 2, Batch 55000, Loss: 0.0313\n",
            "Epoch 2, Batch 55500, Loss: 0.0055\n",
            "Epoch 2, Batch 56000, Loss: 0.0204\n",
            "Epoch 2, Batch 56500, Loss: 0.0061\n",
            "Epoch 2, Batch 57000, Loss: 0.0070\n",
            "Epoch 2, Batch 57500, Loss: 0.0113\n",
            "Epoch 2, Batch 58000, Loss: 0.0251\n",
            "Epoch 2, Batch 58500, Loss: 0.0141\n",
            "Epoch 2, Batch 59000, Loss: 0.0104\n",
            "Epoch 2, Batch 59500, Loss: 0.0042\n",
            "Epoch 2, Batch 60000, Loss: 0.0247\n",
            "Epoch 2, Batch 60500, Loss: 0.0173\n",
            "Epoch 2, Batch 61000, Loss: 0.0386\n",
            "Epoch 2, Batch 61500, Loss: 0.0149\n",
            "Epoch 2, Batch 62000, Loss: 0.0153\n",
            "Epoch 2, Batch 62500, Loss: 0.0214\n",
            "Epoch 2, Batch 63000, Loss: 0.0167\n",
            "Epoch 2, Batch 63500, Loss: 0.0322\n",
            "Epoch 2, Batch 64000, Loss: 0.0096\n",
            "Epoch 2, Batch 64500, Loss: 0.0090\n",
            "Epoch 2, Batch 65000, Loss: 0.0461\n",
            "Epoch 2, Batch 65500, Loss: 0.0319\n",
            "Epoch 2, Batch 66000, Loss: 0.0012\n",
            "Epoch 2, Batch 66500, Loss: 0.0056\n",
            "Epoch 2, Batch 67000, Loss: 0.1848\n",
            "Epoch 2, Batch 67500, Loss: 0.2371\n",
            "Epoch 2, Batch 68000, Loss: 0.0291\n",
            "Epoch 2, Batch 68500, Loss: 0.0023\n",
            "Epoch 2, Batch 69000, Loss: 0.0229\n",
            "Epoch 2, Batch 69500, Loss: 0.0037\n",
            "Epoch 2, Batch 70000, Loss: 0.0150\n",
            "Epoch 2, Batch 70500, Loss: 0.0022\n",
            "Epoch 2, Batch 71000, Loss: 0.1420\n",
            "Epoch 2, Batch 71500, Loss: 0.0084\n",
            "Epoch 2, Batch 72000, Loss: 0.2333\n",
            "Epoch 2, Batch 72500, Loss: 0.0031\n",
            "Epoch 2, Batch 73000, Loss: 0.0123\n",
            "Epoch 2, Batch 73500, Loss: 0.0081\n",
            "Epoch 2, Train Loss: 0.0613, Val Loss: 0.1865\n",
            "Epoch 3, Batch 500, Loss: 0.0037\n",
            "Epoch 3, Batch 1000, Loss: 0.0178\n",
            "Epoch 3, Batch 1500, Loss: 0.0886\n",
            "Epoch 3, Batch 2000, Loss: 0.0940\n",
            "Epoch 3, Batch 2500, Loss: 0.0139\n",
            "Epoch 3, Batch 3000, Loss: 0.0345\n",
            "Epoch 3, Batch 3500, Loss: 0.0270\n",
            "Epoch 3, Batch 4000, Loss: 0.0216\n",
            "Epoch 3, Batch 4500, Loss: 0.0069\n",
            "Epoch 3, Batch 5000, Loss: 0.1604\n",
            "Epoch 3, Batch 5500, Loss: 0.0073\n",
            "Epoch 3, Batch 6000, Loss: 0.1305\n",
            "Epoch 3, Batch 6500, Loss: 0.0720\n",
            "Epoch 3, Batch 7000, Loss: 0.0473\n",
            "Epoch 3, Batch 7500, Loss: 0.0049\n",
            "Epoch 3, Batch 8000, Loss: 0.0445\n",
            "Epoch 3, Batch 8500, Loss: 0.0639\n",
            "Epoch 3, Batch 9000, Loss: 0.0206\n",
            "Epoch 3, Batch 9500, Loss: 0.0836\n",
            "Epoch 3, Batch 10000, Loss: 0.0071\n",
            "Epoch 3, Batch 10500, Loss: 0.0172\n",
            "Epoch 3, Batch 11000, Loss: 0.0216\n",
            "Epoch 3, Batch 11500, Loss: 0.0057\n",
            "Epoch 3, Batch 12000, Loss: 0.0190\n",
            "Epoch 3, Batch 12500, Loss: 0.0261\n",
            "Epoch 3, Batch 13000, Loss: 0.2901\n",
            "Epoch 3, Batch 13500, Loss: 0.0256\n",
            "Epoch 3, Batch 14000, Loss: 0.0063\n",
            "Epoch 3, Batch 14500, Loss: 0.0019\n",
            "Epoch 3, Batch 15000, Loss: 0.4890\n",
            "Epoch 3, Batch 15500, Loss: 0.0260\n",
            "Epoch 3, Batch 16000, Loss: 0.0110\n",
            "Epoch 3, Batch 16500, Loss: 0.0301\n",
            "Epoch 3, Batch 17000, Loss: 0.0431\n",
            "Epoch 3, Batch 17500, Loss: 0.0046\n",
            "Epoch 3, Batch 18000, Loss: 0.5732\n",
            "Epoch 3, Batch 18500, Loss: 0.0214\n",
            "Epoch 3, Batch 19000, Loss: 0.0401\n",
            "Epoch 3, Batch 19500, Loss: 0.0042\n",
            "Epoch 3, Batch 20000, Loss: 0.2737\n",
            "Epoch 3, Batch 20500, Loss: 0.0484\n",
            "Epoch 3, Batch 21000, Loss: 0.0201\n",
            "Epoch 3, Batch 21500, Loss: 0.0118\n",
            "Epoch 3, Batch 22000, Loss: 0.0216\n",
            "Epoch 3, Batch 22500, Loss: 0.0507\n",
            "Epoch 3, Batch 23000, Loss: 0.0497\n",
            "Epoch 3, Batch 23500, Loss: 0.2088\n",
            "Epoch 3, Batch 24000, Loss: 0.0107\n",
            "Epoch 3, Batch 24500, Loss: 0.0480\n",
            "Epoch 3, Batch 25000, Loss: 0.0044\n",
            "Epoch 3, Batch 25500, Loss: 0.0799\n",
            "Epoch 3, Batch 26000, Loss: 0.0085\n",
            "Epoch 3, Batch 26500, Loss: 0.0678\n",
            "Epoch 3, Batch 27000, Loss: 0.0404\n",
            "Epoch 3, Batch 27500, Loss: 0.0290\n",
            "Epoch 3, Batch 28000, Loss: 0.0029\n",
            "Epoch 3, Batch 28500, Loss: 0.0180\n",
            "Epoch 3, Batch 29000, Loss: 0.0095\n",
            "Epoch 3, Batch 29500, Loss: 0.0351\n",
            "Epoch 3, Batch 30000, Loss: 0.0107\n",
            "Epoch 3, Batch 30500, Loss: 0.0177\n",
            "Epoch 3, Batch 31000, Loss: 0.2622\n",
            "Epoch 3, Batch 31500, Loss: 0.1714\n",
            "Epoch 3, Batch 32000, Loss: 0.0523\n",
            "Epoch 3, Batch 32500, Loss: 0.0028\n",
            "Epoch 3, Batch 33000, Loss: 0.0586\n",
            "Epoch 3, Batch 33500, Loss: 0.0053\n",
            "Epoch 3, Batch 34000, Loss: 0.0076\n",
            "Epoch 3, Batch 34500, Loss: 0.0120\n",
            "Epoch 3, Batch 35000, Loss: 0.0653\n",
            "Epoch 3, Batch 35500, Loss: 0.0796\n",
            "Epoch 3, Batch 36000, Loss: 0.0196\n",
            "Epoch 3, Batch 36500, Loss: 0.0253\n",
            "Epoch 3, Batch 37000, Loss: 0.0078\n",
            "Epoch 3, Batch 37500, Loss: 0.0612\n",
            "Epoch 3, Batch 38000, Loss: 0.0034\n",
            "Epoch 3, Batch 38500, Loss: 0.0094\n",
            "Epoch 3, Batch 39000, Loss: 0.0048\n",
            "Epoch 3, Batch 39500, Loss: 0.2046\n",
            "Epoch 3, Batch 40000, Loss: 0.0041\n",
            "Epoch 3, Batch 40500, Loss: 0.0031\n",
            "Epoch 3, Batch 41000, Loss: 0.0152\n",
            "Epoch 3, Batch 41500, Loss: 0.0415\n",
            "Epoch 3, Batch 42000, Loss: 0.0154\n",
            "Epoch 3, Batch 42500, Loss: 0.5315\n",
            "Epoch 3, Batch 43000, Loss: 0.0367\n",
            "Epoch 3, Batch 43500, Loss: 0.0018\n",
            "Epoch 3, Batch 44000, Loss: 0.0358\n",
            "Epoch 3, Batch 44500, Loss: 0.0033\n",
            "Epoch 3, Batch 45000, Loss: 0.0091\n",
            "Epoch 3, Batch 45500, Loss: 0.0107\n",
            "Epoch 3, Batch 46000, Loss: 0.0156\n",
            "Epoch 3, Batch 46500, Loss: 0.0045\n",
            "Epoch 3, Batch 47000, Loss: 0.0101\n",
            "Epoch 3, Batch 47500, Loss: 0.2759\n",
            "Epoch 3, Batch 48000, Loss: 0.0218\n",
            "Epoch 3, Batch 48500, Loss: 0.0108\n",
            "Epoch 3, Batch 49000, Loss: 0.0341\n",
            "Epoch 3, Batch 49500, Loss: 0.0704\n",
            "Epoch 3, Batch 50000, Loss: 0.0020\n",
            "Epoch 3, Batch 50500, Loss: 0.0609\n",
            "Epoch 3, Batch 51000, Loss: 0.0291\n",
            "Epoch 3, Batch 51500, Loss: 0.0273\n",
            "Epoch 3, Batch 52000, Loss: 0.0705\n",
            "Epoch 3, Batch 52500, Loss: 0.0138\n",
            "Epoch 3, Batch 53000, Loss: 0.0205\n",
            "Epoch 3, Batch 53500, Loss: 0.0694\n",
            "Epoch 3, Batch 54000, Loss: 0.0076\n",
            "Epoch 3, Batch 54500, Loss: 0.0056\n",
            "Epoch 3, Batch 55000, Loss: 0.0128\n",
            "Epoch 3, Batch 55500, Loss: 0.0257\n",
            "Epoch 3, Batch 56000, Loss: 0.0244\n",
            "Epoch 3, Batch 56500, Loss: 0.0251\n",
            "Epoch 3, Batch 57000, Loss: 0.0026\n",
            "Epoch 3, Batch 57500, Loss: 0.0954\n",
            "Epoch 3, Batch 58000, Loss: 0.1003\n",
            "Epoch 3, Batch 58500, Loss: 0.1066\n",
            "Epoch 3, Batch 59000, Loss: 0.0293\n",
            "Epoch 3, Batch 59500, Loss: 0.0272\n",
            "Epoch 3, Batch 60000, Loss: 0.0108\n",
            "Epoch 3, Batch 60500, Loss: 0.0269\n",
            "Epoch 3, Batch 61000, Loss: 0.0103\n",
            "Epoch 3, Batch 61500, Loss: 0.0027\n",
            "Epoch 3, Batch 62000, Loss: 0.0145\n",
            "Epoch 3, Batch 62500, Loss: 0.0293\n",
            "Epoch 3, Batch 63000, Loss: 0.0359\n",
            "Epoch 3, Batch 63500, Loss: 0.1024\n",
            "Epoch 3, Batch 64000, Loss: 0.0341\n",
            "Epoch 3, Batch 64500, Loss: 0.0106\n",
            "Epoch 3, Batch 65000, Loss: 0.0783\n",
            "Epoch 3, Batch 65500, Loss: 0.0318\n",
            "Epoch 3, Batch 66000, Loss: 0.0137\n",
            "Epoch 3, Batch 66500, Loss: 0.0092\n",
            "Epoch 3, Batch 67000, Loss: 0.0639\n",
            "Epoch 3, Batch 67500, Loss: 0.0326\n",
            "Epoch 3, Batch 68000, Loss: 0.0144\n",
            "Epoch 3, Batch 68500, Loss: 0.0043\n",
            "Epoch 3, Batch 69000, Loss: 0.0511\n",
            "Epoch 3, Batch 69500, Loss: 0.0462\n",
            "Epoch 3, Batch 70000, Loss: 0.0084\n",
            "Epoch 3, Batch 70500, Loss: 0.0059\n",
            "Epoch 3, Batch 71000, Loss: 0.0016\n",
            "Epoch 3, Batch 71500, Loss: 0.0037\n",
            "Epoch 3, Batch 72000, Loss: 0.0155\n",
            "Epoch 3, Batch 72500, Loss: 0.0104\n",
            "Epoch 3, Batch 73000, Loss: 0.0068\n",
            "Epoch 3, Batch 73500, Loss: 0.0536\n",
            "Epoch 3, Train Loss: 0.0563, Val Loss: 0.1681\n",
            "Epoch 4, Batch 500, Loss: 1.6256\n",
            "Epoch 4, Batch 1000, Loss: 0.0067\n",
            "Epoch 4, Batch 1500, Loss: 0.0327\n",
            "Epoch 4, Batch 2000, Loss: 0.2015\n",
            "Epoch 4, Batch 2500, Loss: 0.0042\n",
            "Epoch 4, Batch 3000, Loss: 0.0030\n",
            "Epoch 4, Batch 3500, Loss: 0.0013\n",
            "Epoch 4, Batch 4000, Loss: 0.0536\n",
            "Epoch 4, Batch 4500, Loss: 0.0995\n",
            "Epoch 4, Batch 5000, Loss: 0.0107\n",
            "Epoch 4, Batch 5500, Loss: 0.0227\n",
            "Epoch 4, Batch 6000, Loss: 0.0052\n",
            "Epoch 4, Batch 6500, Loss: 0.0496\n",
            "Epoch 4, Batch 7000, Loss: 11.4633\n",
            "Epoch 4, Batch 7500, Loss: 0.0168\n",
            "Epoch 4, Batch 8000, Loss: 0.1411\n",
            "Epoch 4, Batch 8500, Loss: 0.0054\n",
            "Epoch 4, Batch 9000, Loss: 0.0084\n",
            "Epoch 4, Batch 9500, Loss: 0.0461\n",
            "Epoch 4, Batch 10000, Loss: 0.0132\n",
            "Epoch 4, Batch 10500, Loss: 0.1844\n",
            "Epoch 4, Batch 11000, Loss: 0.1428\n",
            "Epoch 4, Batch 11500, Loss: 0.0660\n",
            "Epoch 4, Batch 12000, Loss: 0.0812\n",
            "Epoch 4, Batch 12500, Loss: 0.0264\n",
            "Epoch 4, Batch 13000, Loss: 0.0378\n",
            "Epoch 4, Batch 13500, Loss: 0.0253\n",
            "Epoch 4, Batch 14000, Loss: 0.0164\n",
            "Epoch 4, Batch 14500, Loss: 0.0105\n",
            "Epoch 4, Batch 15000, Loss: 0.0120\n",
            "Epoch 4, Batch 15500, Loss: 0.1179\n",
            "Epoch 4, Batch 16000, Loss: 0.0247\n",
            "Epoch 4, Batch 16500, Loss: 0.0241\n",
            "Epoch 4, Batch 17000, Loss: 0.0498\n",
            "Epoch 4, Batch 17500, Loss: 0.0179\n",
            "Epoch 4, Batch 18000, Loss: 0.0047\n",
            "Epoch 4, Batch 18500, Loss: 0.0105\n",
            "Epoch 4, Batch 19000, Loss: 0.0232\n",
            "Epoch 4, Batch 19500, Loss: 0.0205\n",
            "Epoch 4, Batch 20000, Loss: 0.0154\n",
            "Epoch 4, Batch 20500, Loss: 0.0251\n",
            "Epoch 4, Batch 21000, Loss: 0.0364\n",
            "Epoch 4, Batch 21500, Loss: 0.0088\n",
            "Epoch 4, Batch 22000, Loss: 0.0199\n",
            "Epoch 4, Batch 22500, Loss: 0.0254\n",
            "Epoch 4, Batch 23000, Loss: 0.0574\n",
            "Epoch 4, Batch 23500, Loss: 0.0039\n",
            "Epoch 4, Batch 24000, Loss: 0.0038\n",
            "Epoch 4, Batch 24500, Loss: 0.0154\n",
            "Epoch 4, Batch 25000, Loss: 0.0431\n",
            "Epoch 4, Batch 25500, Loss: 0.0223\n",
            "Epoch 4, Batch 26000, Loss: 0.0682\n",
            "Epoch 4, Batch 26500, Loss: 0.0076\n",
            "Epoch 4, Batch 27000, Loss: 0.0253\n",
            "Epoch 4, Batch 27500, Loss: 0.0639\n",
            "Epoch 4, Batch 28000, Loss: 0.0190\n",
            "Epoch 4, Batch 28500, Loss: 0.0213\n",
            "Epoch 4, Batch 29000, Loss: 0.0163\n",
            "Epoch 4, Batch 29500, Loss: 0.0080\n",
            "Epoch 4, Batch 30000, Loss: 0.2237\n",
            "Epoch 4, Batch 30500, Loss: 0.0323\n",
            "Epoch 4, Batch 31000, Loss: 0.0856\n",
            "Epoch 4, Batch 31500, Loss: 0.0044\n",
            "Epoch 4, Batch 32000, Loss: 0.0302\n",
            "Epoch 4, Batch 32500, Loss: 0.0135\n",
            "Epoch 4, Batch 33000, Loss: 0.0078\n",
            "Epoch 4, Batch 33500, Loss: 0.0005\n",
            "Epoch 4, Batch 34000, Loss: 0.1205\n",
            "Epoch 4, Batch 34500, Loss: 0.0314\n",
            "Epoch 4, Batch 35000, Loss: 0.0050\n",
            "Epoch 4, Batch 35500, Loss: 0.0147\n",
            "Epoch 4, Batch 36000, Loss: 0.0286\n",
            "Epoch 4, Batch 36500, Loss: 0.0355\n",
            "Epoch 4, Batch 37000, Loss: 0.0763\n",
            "Epoch 4, Batch 37500, Loss: 0.0327\n",
            "Epoch 4, Batch 38000, Loss: 0.0087\n",
            "Epoch 4, Batch 38500, Loss: 0.0809\n",
            "Epoch 4, Batch 39000, Loss: 0.0136\n",
            "Epoch 4, Batch 39500, Loss: 0.0126\n",
            "Epoch 4, Batch 40000, Loss: 0.0201\n",
            "Epoch 4, Batch 40500, Loss: 0.0704\n",
            "Epoch 4, Batch 41000, Loss: 0.0076\n",
            "Epoch 4, Batch 41500, Loss: 0.0029\n",
            "Epoch 4, Batch 42000, Loss: 0.0241\n",
            "Epoch 4, Batch 42500, Loss: 0.0090\n",
            "Epoch 4, Batch 43000, Loss: 0.0297\n",
            "Epoch 4, Batch 43500, Loss: 0.0418\n",
            "Epoch 4, Batch 44000, Loss: 0.0237\n",
            "Epoch 4, Batch 44500, Loss: 0.0040\n",
            "Epoch 4, Batch 45000, Loss: 0.0014\n",
            "Epoch 4, Batch 45500, Loss: 0.1162\n",
            "Epoch 4, Batch 46000, Loss: 0.0108\n",
            "Epoch 4, Batch 46500, Loss: 0.0520\n",
            "Epoch 4, Batch 47000, Loss: 0.0058\n",
            "Epoch 4, Batch 47500, Loss: 0.0023\n",
            "Epoch 4, Batch 48000, Loss: 0.0224\n",
            "Epoch 4, Batch 48500, Loss: 0.0129\n",
            "Epoch 4, Batch 49000, Loss: 0.0228\n",
            "Epoch 4, Batch 49500, Loss: 0.0132\n",
            "Epoch 4, Batch 50000, Loss: 0.0044\n",
            "Epoch 4, Batch 50500, Loss: 0.0296\n",
            "Epoch 4, Batch 51000, Loss: 0.0011\n",
            "Epoch 4, Batch 51500, Loss: 0.0042\n",
            "Epoch 4, Batch 52000, Loss: 0.0018\n",
            "Epoch 4, Batch 52500, Loss: 0.0882\n",
            "Epoch 4, Batch 53000, Loss: 0.0159\n",
            "Epoch 4, Batch 53500, Loss: 0.0040\n",
            "Epoch 4, Batch 54000, Loss: 0.0157\n",
            "Epoch 4, Batch 54500, Loss: 0.0021\n",
            "Epoch 4, Batch 55000, Loss: 0.0159\n",
            "Epoch 4, Batch 55500, Loss: 0.0146\n",
            "Epoch 4, Batch 56000, Loss: 0.0099\n",
            "Epoch 4, Batch 56500, Loss: 0.0221\n",
            "Epoch 4, Batch 57000, Loss: 0.0088\n",
            "Epoch 4, Batch 57500, Loss: 0.0027\n",
            "Epoch 4, Batch 58000, Loss: 0.0284\n",
            "Epoch 4, Batch 58500, Loss: 0.0119\n",
            "Epoch 4, Batch 59000, Loss: 0.3230\n",
            "Epoch 4, Batch 59500, Loss: 0.0208\n",
            "Epoch 4, Batch 60000, Loss: 0.2950\n",
            "Epoch 4, Batch 60500, Loss: 0.0321\n",
            "Epoch 4, Batch 61000, Loss: 0.0082\n",
            "Epoch 4, Batch 61500, Loss: 0.0508\n",
            "Epoch 4, Batch 62000, Loss: 0.0073\n",
            "Epoch 4, Batch 62500, Loss: 0.0301\n",
            "Epoch 4, Batch 63000, Loss: 0.0545\n",
            "Epoch 4, Batch 63500, Loss: 0.0097\n",
            "Epoch 4, Batch 64000, Loss: 0.0031\n",
            "Epoch 4, Batch 64500, Loss: 0.0089\n",
            "Epoch 4, Batch 65000, Loss: 0.0062\n",
            "Epoch 4, Batch 65500, Loss: 0.0063\n",
            "Epoch 4, Batch 66000, Loss: 0.0095\n",
            "Epoch 4, Batch 66500, Loss: 0.0045\n",
            "Epoch 4, Batch 67000, Loss: 0.0050\n",
            "Epoch 4, Batch 67500, Loss: 0.0158\n",
            "Epoch 4, Batch 68000, Loss: 0.0060\n",
            "Epoch 4, Batch 68500, Loss: 0.0077\n",
            "Epoch 4, Batch 69000, Loss: 0.0447\n",
            "Epoch 4, Batch 69500, Loss: 0.0047\n",
            "Epoch 4, Batch 70000, Loss: 0.0086\n",
            "Epoch 4, Batch 70500, Loss: 0.0058\n",
            "Epoch 4, Batch 71000, Loss: 0.2216\n",
            "Epoch 4, Batch 71500, Loss: 0.0078\n",
            "Epoch 4, Batch 72000, Loss: 0.0254\n",
            "Epoch 4, Batch 72500, Loss: 0.0492\n",
            "Epoch 4, Batch 73000, Loss: 0.0145\n",
            "Epoch 4, Batch 73500, Loss: 0.0136\n",
            "Epoch 4, Train Loss: 0.0528, Val Loss: 0.1603\n",
            "Epoch 5, Batch 500, Loss: 0.0090\n",
            "Epoch 5, Batch 1000, Loss: 0.0253\n",
            "Epoch 5, Batch 1500, Loss: 0.0122\n",
            "Epoch 5, Batch 2000, Loss: 0.0555\n",
            "Epoch 5, Batch 2500, Loss: 0.0112\n",
            "Epoch 5, Batch 3000, Loss: 0.0101\n",
            "Epoch 5, Batch 3500, Loss: 0.0518\n",
            "Epoch 5, Batch 4000, Loss: 0.0055\n",
            "Epoch 5, Batch 4500, Loss: 0.0532\n",
            "Epoch 5, Batch 5000, Loss: 0.0030\n",
            "Epoch 5, Batch 5500, Loss: 0.0075\n",
            "Epoch 5, Batch 6000, Loss: 0.0578\n",
            "Epoch 5, Batch 6500, Loss: 0.0030\n",
            "Epoch 5, Batch 7000, Loss: 0.0233\n",
            "Epoch 5, Batch 7500, Loss: 0.0103\n",
            "Epoch 5, Batch 8000, Loss: 0.0537\n",
            "Epoch 5, Batch 8500, Loss: 0.0043\n",
            "Epoch 5, Batch 9000, Loss: 0.0651\n",
            "Epoch 5, Batch 9500, Loss: 0.0130\n",
            "Epoch 5, Batch 10000, Loss: 0.0364\n",
            "Epoch 5, Batch 10500, Loss: 0.0041\n",
            "Epoch 5, Batch 11000, Loss: 0.0046\n",
            "Epoch 5, Batch 11500, Loss: 0.0167\n",
            "Epoch 5, Batch 12000, Loss: 0.0690\n",
            "Epoch 5, Batch 12500, Loss: 0.0085\n",
            "Epoch 5, Batch 13000, Loss: 0.0043\n",
            "Epoch 5, Batch 13500, Loss: 0.0155\n",
            "Epoch 5, Batch 14000, Loss: 0.0033\n",
            "Epoch 5, Batch 14500, Loss: 0.0009\n",
            "Epoch 5, Batch 15000, Loss: 0.0067\n",
            "Epoch 5, Batch 15500, Loss: 0.0070\n",
            "Epoch 5, Batch 16000, Loss: 0.0020\n",
            "Epoch 5, Batch 16500, Loss: 0.0344\n",
            "Epoch 5, Batch 17000, Loss: 0.0037\n",
            "Epoch 5, Batch 17500, Loss: 0.0517\n",
            "Epoch 5, Batch 18000, Loss: 0.0831\n",
            "Epoch 5, Batch 18500, Loss: 0.0351\n",
            "Epoch 5, Batch 19000, Loss: 0.0068\n",
            "Epoch 5, Batch 19500, Loss: 0.0070\n",
            "Epoch 5, Batch 20000, Loss: 0.0103\n",
            "Epoch 5, Batch 20500, Loss: 0.0784\n",
            "Epoch 5, Batch 21000, Loss: 0.2296\n",
            "Epoch 5, Batch 21500, Loss: 0.0010\n",
            "Epoch 5, Batch 22000, Loss: 0.0166\n",
            "Epoch 5, Batch 22500, Loss: 0.0131\n",
            "Epoch 5, Batch 23000, Loss: 0.0443\n",
            "Epoch 5, Batch 23500, Loss: 0.0052\n",
            "Epoch 5, Batch 24000, Loss: 0.0591\n",
            "Epoch 5, Batch 24500, Loss: 0.0051\n",
            "Epoch 5, Batch 25000, Loss: 0.0013\n",
            "Epoch 5, Batch 25500, Loss: 0.0147\n",
            "Epoch 5, Batch 26000, Loss: 0.0081\n",
            "Epoch 5, Batch 26500, Loss: 0.0587\n",
            "Epoch 5, Batch 27000, Loss: 0.0048\n",
            "Epoch 5, Batch 27500, Loss: 0.0378\n",
            "Epoch 5, Batch 28000, Loss: 0.0028\n",
            "Epoch 5, Batch 28500, Loss: 0.0401\n",
            "Epoch 5, Batch 29000, Loss: 0.0372\n",
            "Epoch 5, Batch 29500, Loss: 0.0123\n",
            "Epoch 5, Batch 30000, Loss: 0.0049\n",
            "Epoch 5, Batch 30500, Loss: 0.0447\n",
            "Epoch 5, Batch 31000, Loss: 0.1057\n",
            "Epoch 5, Batch 31500, Loss: 0.0037\n",
            "Epoch 5, Batch 32000, Loss: 0.0128\n",
            "Epoch 5, Batch 32500, Loss: 0.0312\n",
            "Epoch 5, Batch 33000, Loss: 0.0287\n",
            "Epoch 5, Batch 33500, Loss: 0.0036\n",
            "Epoch 5, Batch 34000, Loss: 0.0660\n",
            "Epoch 5, Batch 34500, Loss: 0.0215\n",
            "Epoch 5, Batch 35000, Loss: 0.0021\n",
            "Epoch 5, Batch 35500, Loss: 0.0248\n",
            "Epoch 5, Batch 36000, Loss: 0.0082\n",
            "Epoch 5, Batch 36500, Loss: 0.1003\n",
            "Epoch 5, Batch 37000, Loss: 0.0386\n",
            "Epoch 5, Batch 37500, Loss: 0.0651\n",
            "Epoch 5, Batch 38000, Loss: 0.0362\n",
            "Epoch 5, Batch 38500, Loss: 0.0313\n",
            "Epoch 5, Batch 39000, Loss: 0.0933\n",
            "Epoch 5, Batch 39500, Loss: 0.0204\n",
            "Epoch 5, Batch 40000, Loss: 0.7912\n",
            "Epoch 5, Batch 40500, Loss: 0.0437\n",
            "Epoch 5, Batch 41000, Loss: 0.0063\n",
            "Epoch 5, Batch 41500, Loss: 0.0062\n",
            "Epoch 5, Batch 42000, Loss: 0.0119\n",
            "Epoch 5, Batch 42500, Loss: 0.0402\n",
            "Epoch 5, Batch 43000, Loss: 0.0850\n",
            "Epoch 5, Batch 43500, Loss: 0.0412\n",
            "Epoch 5, Batch 44000, Loss: 0.0293\n",
            "Epoch 5, Batch 44500, Loss: 0.0861\n",
            "Epoch 5, Batch 45000, Loss: 0.0168\n",
            "Epoch 5, Batch 45500, Loss: 0.0289\n",
            "Epoch 5, Batch 46000, Loss: 0.0106\n",
            "Epoch 5, Batch 46500, Loss: 0.0070\n",
            "Epoch 5, Batch 47000, Loss: 0.0402\n",
            "Epoch 5, Batch 47500, Loss: 0.0105\n",
            "Epoch 5, Batch 48000, Loss: 0.0192\n",
            "Epoch 5, Batch 48500, Loss: 0.0310\n",
            "Epoch 5, Batch 49000, Loss: 0.0459\n",
            "Epoch 5, Batch 49500, Loss: 0.0334\n",
            "Epoch 5, Batch 50000, Loss: 0.0146\n",
            "Epoch 5, Batch 50500, Loss: 0.0075\n",
            "Epoch 5, Batch 51000, Loss: 0.0191\n",
            "Epoch 5, Batch 51500, Loss: 0.0130\n",
            "Epoch 5, Batch 52000, Loss: 0.0045\n",
            "Epoch 5, Batch 52500, Loss: 0.0224\n",
            "Epoch 5, Batch 53000, Loss: 0.0126\n",
            "Epoch 5, Batch 53500, Loss: 0.0280\n",
            "Epoch 5, Batch 54000, Loss: 0.0628\n",
            "Epoch 5, Batch 54500, Loss: 0.0317\n",
            "Epoch 5, Batch 55000, Loss: 0.0349\n",
            "Epoch 5, Batch 55500, Loss: 0.0481\n",
            "Epoch 5, Batch 56000, Loss: 0.0047\n",
            "Epoch 5, Batch 56500, Loss: 0.0893\n",
            "Epoch 5, Batch 57000, Loss: 0.0089\n",
            "Epoch 5, Batch 57500, Loss: 0.0043\n",
            "Epoch 5, Batch 58000, Loss: 0.0294\n",
            "Epoch 5, Batch 58500, Loss: 0.0083\n",
            "Epoch 5, Batch 59000, Loss: 0.0581\n",
            "Epoch 5, Batch 59500, Loss: 0.0276\n",
            "Epoch 5, Batch 60000, Loss: 0.0020\n",
            "Epoch 5, Batch 60500, Loss: 0.0360\n",
            "Epoch 5, Batch 61000, Loss: 0.0328\n",
            "Epoch 5, Batch 61500, Loss: 0.0023\n",
            "Epoch 5, Batch 62000, Loss: 0.0667\n",
            "Epoch 5, Batch 62500, Loss: 0.0623\n",
            "Epoch 5, Batch 63000, Loss: 0.0015\n",
            "Epoch 5, Batch 63500, Loss: 0.0255\n",
            "Epoch 5, Batch 64000, Loss: 0.0133\n",
            "Epoch 5, Batch 64500, Loss: 0.0342\n",
            "Epoch 5, Batch 65000, Loss: 0.0181\n",
            "Epoch 5, Batch 65500, Loss: 0.0033\n",
            "Epoch 5, Batch 66000, Loss: 0.0073\n",
            "Epoch 5, Batch 66500, Loss: 0.0064\n",
            "Epoch 5, Batch 67000, Loss: 0.0045\n",
            "Epoch 5, Batch 67500, Loss: 0.0027\n",
            "Epoch 5, Batch 68000, Loss: 0.1564\n",
            "Epoch 5, Batch 68500, Loss: 0.0698\n",
            "Epoch 5, Batch 69000, Loss: 0.0808\n",
            "Epoch 5, Batch 69500, Loss: 0.0534\n",
            "Epoch 5, Batch 70000, Loss: 0.0115\n",
            "Epoch 5, Batch 70500, Loss: 0.0711\n",
            "Epoch 5, Batch 71000, Loss: 0.1010\n",
            "Epoch 5, Batch 71500, Loss: 0.0393\n",
            "Epoch 5, Batch 72000, Loss: 0.0023\n",
            "Epoch 5, Batch 72500, Loss: 0.0013\n",
            "Epoch 5, Batch 73000, Loss: 0.0084\n",
            "Epoch 5, Batch 73500, Loss: 0.0102\n",
            "Epoch 5, Train Loss: 0.0501, Val Loss: 0.1539\n",
            "Epoch 6, Batch 500, Loss: 0.0085\n",
            "Epoch 6, Batch 1000, Loss: 0.0255\n",
            "Epoch 6, Batch 1500, Loss: 0.0038\n",
            "Epoch 6, Batch 2000, Loss: 0.0516\n",
            "Epoch 6, Batch 2500, Loss: 0.0065\n",
            "Epoch 6, Batch 3000, Loss: 0.0228\n",
            "Epoch 6, Batch 3500, Loss: 0.0275\n",
            "Epoch 6, Batch 4000, Loss: 0.0022\n",
            "Epoch 6, Batch 4500, Loss: 0.0248\n",
            "Epoch 6, Batch 5000, Loss: 0.0025\n",
            "Epoch 6, Batch 5500, Loss: 0.0294\n",
            "Epoch 6, Batch 6000, Loss: 0.0290\n",
            "Epoch 6, Batch 6500, Loss: 0.0276\n",
            "Epoch 6, Batch 7000, Loss: 0.0196\n",
            "Epoch 6, Batch 7500, Loss: 0.0099\n",
            "Epoch 6, Batch 8000, Loss: 0.1102\n",
            "Epoch 6, Batch 8500, Loss: 0.0358\n",
            "Epoch 6, Batch 9000, Loss: 0.0274\n",
            "Epoch 6, Batch 9500, Loss: 0.0109\n",
            "Epoch 6, Batch 10000, Loss: 0.0090\n",
            "Epoch 6, Batch 10500, Loss: 0.0116\n",
            "Epoch 6, Batch 11000, Loss: 0.0067\n",
            "Epoch 6, Batch 11500, Loss: 0.0218\n",
            "Epoch 6, Batch 12000, Loss: 0.0497\n",
            "Epoch 6, Batch 12500, Loss: 0.0421\n",
            "Epoch 6, Batch 13000, Loss: 0.0016\n",
            "Epoch 6, Batch 13500, Loss: 0.0133\n",
            "Epoch 6, Batch 14000, Loss: 0.0036\n",
            "Epoch 6, Batch 14500, Loss: 0.0015\n",
            "Epoch 6, Batch 15000, Loss: 0.0120\n",
            "Epoch 6, Batch 15500, Loss: 0.0127\n",
            "Epoch 6, Batch 16000, Loss: 0.0182\n",
            "Epoch 6, Batch 16500, Loss: 0.0068\n",
            "Epoch 6, Batch 17000, Loss: 0.0087\n",
            "Epoch 6, Batch 17500, Loss: 0.0225\n",
            "Epoch 6, Batch 18000, Loss: 0.0580\n",
            "Epoch 6, Batch 18500, Loss: 0.0019\n",
            "Epoch 6, Batch 19000, Loss: 0.0072\n",
            "Epoch 6, Batch 19500, Loss: 0.0015\n",
            "Epoch 6, Batch 20000, Loss: 0.0123\n",
            "Epoch 6, Batch 20500, Loss: 0.0231\n",
            "Epoch 6, Batch 21000, Loss: 0.0114\n",
            "Epoch 6, Batch 21500, Loss: 0.1087\n",
            "Epoch 6, Batch 22000, Loss: 0.0045\n",
            "Epoch 6, Batch 22500, Loss: 0.0109\n",
            "Epoch 6, Batch 23000, Loss: 0.0820\n",
            "Epoch 6, Batch 23500, Loss: 0.0024\n",
            "Epoch 6, Batch 24000, Loss: 0.0164\n",
            "Epoch 6, Batch 24500, Loss: 0.1076\n",
            "Epoch 6, Batch 25000, Loss: 0.0212\n",
            "Epoch 6, Batch 25500, Loss: 0.0320\n",
            "Epoch 6, Batch 26000, Loss: 0.0049\n",
            "Epoch 6, Batch 26500, Loss: 0.0061\n",
            "Epoch 6, Batch 27000, Loss: 0.1008\n",
            "Epoch 6, Batch 27500, Loss: 0.0085\n",
            "Epoch 6, Batch 28000, Loss: 0.0068\n",
            "Epoch 6, Batch 28500, Loss: 0.0127\n",
            "Epoch 6, Batch 29000, Loss: 0.0179\n",
            "Epoch 6, Batch 29500, Loss: 0.0087\n",
            "Epoch 6, Batch 30000, Loss: 0.0179\n",
            "Epoch 6, Batch 30500, Loss: 0.0257\n",
            "Epoch 6, Batch 31000, Loss: 0.0436\n",
            "Epoch 6, Batch 31500, Loss: 0.0109\n",
            "Epoch 6, Batch 32000, Loss: 0.0240\n",
            "Epoch 6, Batch 32500, Loss: 0.0117\n",
            "Epoch 6, Batch 33000, Loss: 0.0722\n",
            "Epoch 6, Batch 33500, Loss: 0.0124\n",
            "Epoch 6, Batch 34000, Loss: 0.0439\n",
            "Epoch 6, Batch 34500, Loss: 0.0024\n",
            "Epoch 6, Batch 35000, Loss: 0.0564\n",
            "Epoch 6, Batch 35500, Loss: 0.0072\n",
            "Epoch 6, Batch 36000, Loss: 0.0020\n",
            "Epoch 6, Batch 36500, Loss: 0.0067\n",
            "Epoch 6, Batch 37000, Loss: 0.0479\n",
            "Epoch 6, Batch 37500, Loss: 0.0023\n",
            "Epoch 6, Batch 38000, Loss: 0.0337\n",
            "Epoch 6, Batch 38500, Loss: 0.0301\n",
            "Epoch 6, Batch 39000, Loss: 0.0067\n",
            "Epoch 6, Batch 39500, Loss: 0.0060\n",
            "Epoch 6, Batch 40000, Loss: 0.0433\n",
            "Epoch 6, Batch 40500, Loss: 0.0210\n",
            "Epoch 6, Batch 41000, Loss: 0.0234\n",
            "Epoch 6, Batch 41500, Loss: 0.0146\n",
            "Epoch 6, Batch 42000, Loss: 0.0033\n",
            "Epoch 6, Batch 42500, Loss: 0.0092\n",
            "Epoch 6, Batch 43000, Loss: 0.0079\n",
            "Epoch 6, Batch 43500, Loss: 0.0131\n",
            "Epoch 6, Batch 44000, Loss: 0.0118\n",
            "Epoch 6, Batch 44500, Loss: 0.0203\n",
            "Epoch 6, Batch 45000, Loss: 0.0114\n",
            "Epoch 6, Batch 45500, Loss: 0.0030\n",
            "Epoch 6, Batch 46000, Loss: 0.0590\n",
            "Epoch 6, Batch 46500, Loss: 0.0350\n",
            "Epoch 6, Batch 47000, Loss: 0.0175\n",
            "Epoch 6, Batch 47500, Loss: 0.0091\n",
            "Epoch 6, Batch 48000, Loss: 0.0070\n",
            "Epoch 6, Batch 48500, Loss: 0.0317\n",
            "Epoch 6, Batch 49000, Loss: 0.0172\n",
            "Epoch 6, Batch 49500, Loss: 0.0122\n",
            "Epoch 6, Batch 50000, Loss: 0.0024\n",
            "Epoch 6, Batch 50500, Loss: 0.0407\n",
            "Epoch 6, Batch 51000, Loss: 0.0033\n",
            "Epoch 6, Batch 51500, Loss: 0.0039\n",
            "Epoch 6, Batch 52000, Loss: 0.0070\n",
            "Epoch 6, Batch 52500, Loss: 0.0285\n",
            "Epoch 6, Batch 53000, Loss: 0.0129\n",
            "Epoch 6, Batch 53500, Loss: 0.0061\n",
            "Epoch 6, Batch 54000, Loss: 0.0305\n",
            "Epoch 6, Batch 54500, Loss: 0.0265\n",
            "Epoch 6, Batch 55000, Loss: 0.0969\n",
            "Epoch 6, Batch 55500, Loss: 0.0247\n",
            "Epoch 6, Batch 56000, Loss: 0.0076\n",
            "Epoch 6, Batch 56500, Loss: 0.0283\n",
            "Epoch 6, Batch 57000, Loss: 0.0048\n",
            "Epoch 6, Batch 57500, Loss: 0.0054\n",
            "Epoch 6, Batch 58000, Loss: 0.0075\n",
            "Epoch 6, Batch 58500, Loss: 0.0264\n",
            "Epoch 6, Batch 59000, Loss: 0.0214\n",
            "Epoch 6, Batch 59500, Loss: 0.0309\n",
            "Epoch 6, Batch 60000, Loss: 0.0259\n",
            "Epoch 6, Batch 60500, Loss: 0.0720\n",
            "Epoch 6, Batch 61000, Loss: 0.0083\n",
            "Epoch 6, Batch 61500, Loss: 0.0161\n",
            "Epoch 6, Batch 62000, Loss: 0.0311\n",
            "Epoch 6, Batch 62500, Loss: 0.0371\n",
            "Epoch 6, Batch 63000, Loss: 0.0136\n",
            "Epoch 6, Batch 63500, Loss: 0.0025\n",
            "Epoch 6, Batch 64000, Loss: 0.0062\n",
            "Epoch 6, Batch 64500, Loss: 0.0094\n",
            "Epoch 6, Batch 65000, Loss: 0.0214\n",
            "Epoch 6, Batch 65500, Loss: 0.0110\n",
            "Epoch 6, Batch 66000, Loss: 0.0040\n",
            "Epoch 6, Batch 66500, Loss: 0.0619\n",
            "Epoch 6, Batch 67000, Loss: 0.0329\n",
            "Epoch 6, Batch 67500, Loss: 0.0336\n",
            "Epoch 6, Batch 68000, Loss: 0.0164\n",
            "Epoch 6, Batch 68500, Loss: 0.0200\n",
            "Epoch 6, Batch 69000, Loss: 0.0093\n",
            "Epoch 6, Batch 69500, Loss: 0.0247\n",
            "Epoch 6, Batch 70000, Loss: 0.0049\n",
            "Epoch 6, Batch 70500, Loss: 0.0137\n",
            "Epoch 6, Batch 71000, Loss: 0.0191\n",
            "Epoch 6, Batch 71500, Loss: 0.0200\n",
            "Epoch 6, Batch 72000, Loss: 0.0435\n",
            "Epoch 6, Batch 72500, Loss: 0.0141\n",
            "Epoch 6, Batch 73000, Loss: 0.0031\n",
            "Epoch 6, Batch 73500, Loss: 0.0046\n",
            "Epoch 6, Train Loss: 0.0479, Val Loss: 0.1600\n",
            "Epoch 7, Batch 500, Loss: 0.0190\n",
            "Epoch 7, Batch 1000, Loss: 1.1180\n",
            "Epoch 7, Batch 1500, Loss: 0.0042\n",
            "Epoch 7, Batch 2000, Loss: 0.0264\n",
            "Epoch 7, Batch 2500, Loss: 0.0304\n",
            "Epoch 7, Batch 3000, Loss: 0.0181\n",
            "Epoch 7, Batch 3500, Loss: 0.1598\n",
            "Epoch 7, Batch 4000, Loss: 0.0024\n",
            "Epoch 7, Batch 4500, Loss: 0.0042\n",
            "Epoch 7, Batch 5000, Loss: 0.0211\n",
            "Epoch 7, Batch 5500, Loss: 0.0200\n",
            "Epoch 7, Batch 6000, Loss: 0.0245\n",
            "Epoch 7, Batch 6500, Loss: 0.0278\n",
            "Epoch 7, Batch 7000, Loss: 0.2283\n",
            "Epoch 7, Batch 7500, Loss: 0.0017\n",
            "Epoch 7, Batch 8000, Loss: 0.0024\n",
            "Epoch 7, Batch 8500, Loss: 0.0034\n",
            "Epoch 7, Batch 9000, Loss: 0.0231\n",
            "Epoch 7, Batch 9500, Loss: 0.0349\n",
            "Epoch 7, Batch 10000, Loss: 0.0229\n",
            "Epoch 7, Batch 10500, Loss: 0.0308\n",
            "Epoch 7, Batch 11000, Loss: 0.0016\n",
            "Epoch 7, Batch 11500, Loss: 0.0161\n",
            "Epoch 7, Batch 12000, Loss: 0.0288\n",
            "Epoch 7, Batch 12500, Loss: 0.0328\n",
            "Epoch 7, Batch 13000, Loss: 0.0285\n",
            "Epoch 7, Batch 13500, Loss: 0.0064\n",
            "Epoch 7, Batch 14000, Loss: 0.0064\n",
            "Epoch 7, Batch 14500, Loss: 0.0059\n",
            "Epoch 7, Batch 15000, Loss: 0.0026\n",
            "Epoch 7, Batch 15500, Loss: 0.0027\n",
            "Epoch 7, Batch 16000, Loss: 0.0080\n",
            "Epoch 7, Batch 16500, Loss: 0.0168\n",
            "Epoch 7, Batch 17000, Loss: 0.0466\n",
            "Epoch 7, Batch 17500, Loss: 0.0142\n",
            "Epoch 7, Batch 18000, Loss: 0.0558\n",
            "Epoch 7, Batch 18500, Loss: 2.0342\n",
            "Epoch 7, Batch 19000, Loss: 0.0229\n",
            "Epoch 7, Batch 19500, Loss: 0.0148\n",
            "Epoch 7, Batch 20000, Loss: 0.0233\n",
            "Epoch 7, Batch 20500, Loss: 0.0036\n",
            "Epoch 7, Batch 21000, Loss: 0.0125\n",
            "Epoch 7, Batch 21500, Loss: 0.0039\n",
            "Epoch 7, Batch 22000, Loss: 0.0683\n",
            "Epoch 7, Batch 22500, Loss: 0.0248\n",
            "Epoch 7, Batch 23000, Loss: 0.0052\n",
            "Epoch 7, Batch 23500, Loss: 0.0105\n",
            "Epoch 7, Batch 24000, Loss: 0.0041\n",
            "Epoch 7, Batch 24500, Loss: 0.0057\n",
            "Epoch 7, Batch 25000, Loss: 0.0231\n",
            "Epoch 7, Batch 25500, Loss: 0.0271\n",
            "Epoch 7, Batch 26000, Loss: 0.0263\n",
            "Epoch 7, Batch 26500, Loss: 0.0404\n",
            "Epoch 7, Batch 27000, Loss: 0.0099\n",
            "Epoch 7, Batch 27500, Loss: 0.0346\n",
            "Epoch 7, Batch 28000, Loss: 0.0038\n",
            "Epoch 7, Batch 28500, Loss: 0.0018\n",
            "Epoch 7, Batch 29000, Loss: 0.0353\n",
            "Epoch 7, Batch 29500, Loss: 0.0095\n",
            "Epoch 7, Batch 30000, Loss: 0.0317\n",
            "Epoch 7, Batch 30500, Loss: 0.0185\n",
            "Epoch 7, Batch 31000, Loss: 0.0170\n",
            "Epoch 7, Batch 31500, Loss: 0.0210\n",
            "Epoch 7, Batch 32000, Loss: 0.0167\n",
            "Epoch 7, Batch 32500, Loss: 0.0109\n",
            "Epoch 7, Batch 33000, Loss: 0.0423\n",
            "Epoch 7, Batch 33500, Loss: 0.0049\n",
            "Epoch 7, Batch 34000, Loss: 0.0196\n",
            "Epoch 7, Batch 34500, Loss: 0.0015\n",
            "Epoch 7, Batch 35000, Loss: 0.0488\n",
            "Epoch 7, Batch 35500, Loss: 0.0229\n",
            "Epoch 7, Batch 36000, Loss: 0.0155\n",
            "Epoch 7, Batch 36500, Loss: 0.0064\n",
            "Epoch 7, Batch 37000, Loss: 0.0102\n",
            "Epoch 7, Batch 37500, Loss: 0.0070\n",
            "Epoch 7, Batch 38000, Loss: 0.0049\n",
            "Epoch 7, Batch 38500, Loss: 0.1040\n",
            "Epoch 7, Batch 39000, Loss: 0.0081\n",
            "Epoch 7, Batch 39500, Loss: 0.1023\n",
            "Epoch 7, Batch 40000, Loss: 0.0153\n",
            "Epoch 7, Batch 40500, Loss: 0.0228\n",
            "Epoch 7, Batch 41000, Loss: 0.0226\n",
            "Epoch 7, Batch 41500, Loss: 0.0046\n",
            "Epoch 7, Batch 42000, Loss: 0.0206\n",
            "Epoch 7, Batch 42500, Loss: 0.0190\n",
            "Epoch 7, Batch 43000, Loss: 0.0087\n",
            "Epoch 7, Batch 43500, Loss: 0.0023\n",
            "Epoch 7, Batch 44000, Loss: 0.0146\n",
            "Epoch 7, Batch 44500, Loss: 0.1025\n",
            "Epoch 7, Batch 45000, Loss: 0.0270\n",
            "Epoch 7, Batch 45500, Loss: 0.0218\n",
            "Epoch 7, Batch 46000, Loss: 0.0167\n",
            "Epoch 7, Batch 46500, Loss: 0.0440\n",
            "Epoch 7, Batch 47000, Loss: 0.0166\n",
            "Epoch 7, Batch 47500, Loss: 0.1193\n",
            "Epoch 7, Batch 48000, Loss: 0.0044\n",
            "Epoch 7, Batch 48500, Loss: 0.0404\n",
            "Epoch 7, Batch 49000, Loss: 0.0103\n",
            "Epoch 7, Batch 49500, Loss: 0.0228\n",
            "Epoch 7, Batch 50000, Loss: 0.0061\n",
            "Epoch 7, Batch 50500, Loss: 0.0179\n",
            "Epoch 7, Batch 51000, Loss: 0.0031\n",
            "Epoch 7, Batch 51500, Loss: 0.0102\n",
            "Epoch 7, Batch 52000, Loss: 0.0081\n",
            "Epoch 7, Batch 52500, Loss: 0.0318\n",
            "Epoch 7, Batch 53000, Loss: 0.0036\n",
            "Epoch 7, Batch 53500, Loss: 0.1073\n",
            "Epoch 7, Batch 54000, Loss: 0.0012\n",
            "Epoch 7, Batch 54500, Loss: 0.0366\n",
            "Epoch 7, Batch 55000, Loss: 0.0061\n",
            "Epoch 7, Batch 55500, Loss: 0.1744\n",
            "Epoch 7, Batch 56000, Loss: 0.0086\n",
            "Epoch 7, Batch 56500, Loss: 0.0219\n",
            "Epoch 7, Batch 57000, Loss: 0.0178\n",
            "Epoch 7, Batch 57500, Loss: 0.0398\n",
            "Epoch 7, Batch 58000, Loss: 0.0019\n",
            "Epoch 7, Batch 58500, Loss: 0.0327\n",
            "Epoch 7, Batch 59000, Loss: 0.0033\n",
            "Epoch 7, Batch 59500, Loss: 0.0038\n",
            "Epoch 7, Batch 60000, Loss: 0.0252\n",
            "Epoch 7, Batch 60500, Loss: 0.0183\n",
            "Epoch 7, Batch 61000, Loss: 0.0026\n",
            "Epoch 7, Batch 61500, Loss: 0.0062\n",
            "Epoch 7, Batch 62000, Loss: 0.0146\n",
            "Epoch 7, Batch 62500, Loss: 0.0073\n",
            "Epoch 7, Batch 63000, Loss: 0.0112\n",
            "Epoch 7, Batch 63500, Loss: 0.0867\n",
            "Epoch 7, Batch 64000, Loss: 0.0204\n",
            "Epoch 7, Batch 64500, Loss: 0.0491\n",
            "Epoch 7, Batch 65000, Loss: 0.0742\n",
            "Epoch 7, Batch 65500, Loss: 0.0135\n",
            "Epoch 7, Batch 66000, Loss: 0.0143\n",
            "Epoch 7, Batch 66500, Loss: 0.0098\n",
            "Epoch 7, Batch 67000, Loss: 0.0176\n",
            "Epoch 7, Batch 67500, Loss: 0.0074\n",
            "Epoch 7, Batch 68000, Loss: 0.0646\n",
            "Epoch 7, Batch 68500, Loss: 0.0278\n",
            "Epoch 7, Batch 69000, Loss: 0.0953\n",
            "Epoch 7, Batch 69500, Loss: 0.0611\n",
            "Epoch 7, Batch 70000, Loss: 0.0193\n",
            "Epoch 7, Batch 70500, Loss: 0.1279\n",
            "Epoch 7, Batch 71000, Loss: 0.0113\n",
            "Epoch 7, Batch 71500, Loss: 0.0181\n",
            "Epoch 7, Batch 72000, Loss: 0.0260\n",
            "Epoch 7, Batch 72500, Loss: 0.0365\n",
            "Epoch 7, Batch 73000, Loss: 0.0072\n",
            "Epoch 7, Batch 73500, Loss: 0.0129\n",
            "Epoch 7, Train Loss: 0.0464, Val Loss: 0.1561\n",
            "Epoch 8, Batch 500, Loss: 0.0130\n",
            "Epoch 8, Batch 1000, Loss: 0.0052\n",
            "Epoch 8, Batch 1500, Loss: 0.0402\n",
            "Epoch 8, Batch 2000, Loss: 0.0115\n",
            "Epoch 8, Batch 2500, Loss: 0.0060\n",
            "Epoch 8, Batch 3000, Loss: 0.0909\n",
            "Epoch 8, Batch 3500, Loss: 0.1026\n",
            "Epoch 8, Batch 4000, Loss: 0.0077\n",
            "Epoch 8, Batch 4500, Loss: 0.0066\n",
            "Epoch 8, Batch 5000, Loss: 0.0202\n",
            "Epoch 8, Batch 5500, Loss: 0.2012\n",
            "Epoch 8, Batch 6000, Loss: 0.0262\n",
            "Epoch 8, Batch 6500, Loss: 0.1311\n",
            "Epoch 8, Batch 7000, Loss: 0.0023\n",
            "Epoch 8, Batch 7500, Loss: 0.0137\n",
            "Epoch 8, Batch 8000, Loss: 0.0091\n",
            "Epoch 8, Batch 8500, Loss: 0.0241\n",
            "Epoch 8, Batch 9000, Loss: 0.0587\n",
            "Epoch 8, Batch 9500, Loss: 0.0368\n",
            "Epoch 8, Batch 10000, Loss: 0.0022\n",
            "Epoch 8, Batch 10500, Loss: 0.0212\n",
            "Epoch 8, Batch 11000, Loss: 0.0359\n",
            "Epoch 8, Batch 11500, Loss: 0.0240\n",
            "Epoch 8, Batch 12000, Loss: 0.0683\n",
            "Epoch 8, Batch 12500, Loss: 0.0184\n",
            "Epoch 8, Batch 13000, Loss: 0.0265\n",
            "Epoch 8, Batch 13500, Loss: 0.0626\n",
            "Epoch 8, Batch 14000, Loss: 0.0076\n",
            "Epoch 8, Batch 14500, Loss: 0.0158\n",
            "Epoch 8, Batch 15000, Loss: 0.1024\n",
            "Epoch 8, Batch 15500, Loss: 0.0011\n",
            "Epoch 8, Batch 16000, Loss: 0.0656\n",
            "Epoch 8, Batch 16500, Loss: 0.0866\n",
            "Epoch 8, Batch 17000, Loss: 0.0246\n",
            "Epoch 8, Batch 17500, Loss: 0.0119\n",
            "Epoch 8, Batch 18000, Loss: 0.0035\n",
            "Epoch 8, Batch 18500, Loss: 0.0543\n",
            "Epoch 8, Batch 19000, Loss: 0.0859\n",
            "Epoch 8, Batch 19500, Loss: 0.0087\n",
            "Epoch 8, Batch 20000, Loss: 0.0052\n",
            "Epoch 8, Batch 20500, Loss: 0.0381\n",
            "Epoch 8, Batch 21000, Loss: 0.0483\n",
            "Epoch 8, Batch 21500, Loss: 0.0233\n",
            "Epoch 8, Batch 22000, Loss: 0.0151\n",
            "Epoch 8, Batch 22500, Loss: 0.0767\n",
            "Epoch 8, Batch 23000, Loss: 0.0519\n",
            "Epoch 8, Batch 23500, Loss: 0.0095\n",
            "Epoch 8, Batch 24000, Loss: 0.0727\n",
            "Epoch 8, Batch 24500, Loss: 0.0927\n",
            "Epoch 8, Batch 25000, Loss: 0.0030\n",
            "Epoch 8, Batch 25500, Loss: 0.0244\n",
            "Epoch 8, Batch 26000, Loss: 0.0132\n",
            "Epoch 8, Batch 26500, Loss: 0.0038\n",
            "Epoch 8, Batch 27000, Loss: 0.0110\n",
            "Epoch 8, Batch 27500, Loss: 0.0109\n",
            "Epoch 8, Batch 28000, Loss: 0.0327\n",
            "Epoch 8, Batch 28500, Loss: 0.0130\n",
            "Epoch 8, Batch 29000, Loss: 0.0040\n",
            "Epoch 8, Batch 29500, Loss: 0.0510\n",
            "Epoch 8, Batch 30000, Loss: 0.0022\n",
            "Epoch 8, Batch 30500, Loss: 0.1889\n",
            "Epoch 8, Batch 31000, Loss: 0.0791\n",
            "Epoch 8, Batch 31500, Loss: 0.0297\n",
            "Epoch 8, Batch 32000, Loss: 0.0247\n",
            "Epoch 8, Batch 32500, Loss: 0.1118\n",
            "Epoch 8, Batch 33000, Loss: 0.0114\n",
            "Epoch 8, Batch 33500, Loss: 0.0288\n",
            "Epoch 8, Batch 34000, Loss: 0.0314\n",
            "Epoch 8, Batch 34500, Loss: 0.0312\n",
            "Epoch 8, Batch 35000, Loss: 0.0690\n",
            "Epoch 8, Batch 35500, Loss: 0.0105\n",
            "Epoch 8, Batch 36000, Loss: 0.0039\n",
            "Epoch 8, Batch 36500, Loss: 0.0316\n",
            "Epoch 8, Batch 37000, Loss: 0.0051\n",
            "Epoch 8, Batch 37500, Loss: 0.0089\n",
            "Epoch 8, Batch 38000, Loss: 0.0024\n",
            "Epoch 8, Batch 38500, Loss: 0.0781\n",
            "Epoch 8, Batch 39000, Loss: 0.0116\n",
            "Epoch 8, Batch 39500, Loss: 0.0069\n",
            "Epoch 8, Batch 40000, Loss: 0.0054\n",
            "Epoch 8, Batch 40500, Loss: 0.0068\n",
            "Epoch 8, Batch 41000, Loss: 0.0060\n",
            "Epoch 8, Batch 41500, Loss: 0.0057\n",
            "Epoch 8, Batch 42000, Loss: 0.0103\n",
            "Epoch 8, Batch 42500, Loss: 0.0066\n",
            "Epoch 8, Batch 43000, Loss: 0.0147\n",
            "Epoch 8, Batch 43500, Loss: 0.1736\n",
            "Epoch 8, Batch 44000, Loss: 0.0970\n",
            "Epoch 8, Batch 44500, Loss: 0.0394\n",
            "Epoch 8, Batch 45000, Loss: 0.0111\n",
            "Epoch 8, Batch 45500, Loss: 0.0012\n",
            "Epoch 8, Batch 46000, Loss: 0.0063\n",
            "Epoch 8, Batch 46500, Loss: 0.0217\n",
            "Epoch 8, Batch 47000, Loss: 0.0278\n",
            "Epoch 8, Batch 47500, Loss: 0.0141\n",
            "Epoch 8, Batch 48000, Loss: 0.0027\n",
            "Epoch 8, Batch 48500, Loss: 0.0240\n",
            "Epoch 8, Batch 49000, Loss: 0.0440\n",
            "Epoch 8, Batch 49500, Loss: 0.0126\n",
            "Epoch 8, Batch 50000, Loss: 0.0162\n",
            "Epoch 8, Batch 50500, Loss: 0.0099\n",
            "Epoch 8, Batch 51000, Loss: 0.0049\n",
            "Epoch 8, Batch 51500, Loss: 0.0171\n",
            "Epoch 8, Batch 52000, Loss: 0.0732\n",
            "Epoch 8, Batch 52500, Loss: 0.0072\n",
            "Epoch 8, Batch 53000, Loss: 0.0434\n",
            "Epoch 8, Batch 53500, Loss: 0.0019\n",
            "Epoch 8, Batch 54000, Loss: 0.0059\n",
            "Epoch 8, Batch 54500, Loss: 0.0069\n",
            "Epoch 8, Batch 55000, Loss: 0.0308\n",
            "Epoch 8, Batch 55500, Loss: 0.0027\n",
            "Epoch 8, Batch 56000, Loss: 0.0503\n",
            "Epoch 8, Batch 56500, Loss: 0.0063\n",
            "Epoch 8, Batch 57000, Loss: 0.0305\n",
            "Epoch 8, Batch 57500, Loss: 0.0306\n",
            "Epoch 8, Batch 58000, Loss: 0.0550\n",
            "Epoch 8, Batch 58500, Loss: 0.0207\n",
            "Epoch 8, Batch 59000, Loss: 0.0054\n",
            "Epoch 8, Batch 59500, Loss: 0.0034\n",
            "Epoch 8, Batch 60000, Loss: 0.0086\n",
            "Epoch 8, Batch 60500, Loss: 0.0201\n",
            "Epoch 8, Batch 61000, Loss: 0.0389\n",
            "Epoch 8, Batch 61500, Loss: 0.0086\n",
            "Epoch 8, Batch 62000, Loss: 0.0222\n",
            "Epoch 8, Batch 62500, Loss: 0.0229\n",
            "Epoch 8, Batch 63000, Loss: 0.0154\n",
            "Epoch 8, Batch 63500, Loss: 0.0849\n",
            "Epoch 8, Batch 64000, Loss: 0.0247\n",
            "Epoch 8, Batch 64500, Loss: 0.0008\n",
            "Epoch 8, Batch 65000, Loss: 0.0309\n",
            "Epoch 8, Batch 65500, Loss: 0.0139\n",
            "Epoch 8, Batch 66000, Loss: 0.0101\n",
            "Epoch 8, Batch 66500, Loss: 0.0157\n",
            "Epoch 8, Batch 67000, Loss: 0.0012\n",
            "Epoch 8, Batch 67500, Loss: 0.0063\n",
            "Epoch 8, Batch 68000, Loss: 0.0044\n",
            "Epoch 8, Batch 68500, Loss: 0.0371\n",
            "Epoch 8, Batch 69000, Loss: 0.0142\n",
            "Epoch 8, Batch 69500, Loss: 0.0233\n",
            "Epoch 8, Batch 70000, Loss: 0.0020\n",
            "Epoch 8, Batch 70500, Loss: 0.0729\n",
            "Epoch 8, Batch 71000, Loss: 0.0079\n",
            "Epoch 8, Batch 71500, Loss: 0.0381\n",
            "Epoch 8, Batch 72000, Loss: 0.0755\n",
            "Epoch 8, Batch 72500, Loss: 0.0063\n",
            "Epoch 8, Batch 73000, Loss: 0.0019\n",
            "Epoch 8, Batch 73500, Loss: 0.0297\n",
            "Epoch 8, Train Loss: 0.0452, Val Loss: 0.1451\n",
            "Epoch 9, Batch 500, Loss: 0.0204\n",
            "Epoch 9, Batch 1000, Loss: 0.0112\n",
            "Epoch 9, Batch 1500, Loss: 0.0322\n",
            "Epoch 9, Batch 2000, Loss: 0.0532\n",
            "Epoch 9, Batch 2500, Loss: 0.0234\n",
            "Epoch 9, Batch 3000, Loss: 0.0102\n",
            "Epoch 9, Batch 3500, Loss: 0.0074\n",
            "Epoch 9, Batch 4000, Loss: 0.0114\n",
            "Epoch 9, Batch 4500, Loss: 0.0152\n",
            "Epoch 9, Batch 5000, Loss: 0.0068\n",
            "Epoch 9, Batch 5500, Loss: 0.1874\n",
            "Epoch 9, Batch 6000, Loss: 0.0140\n",
            "Epoch 9, Batch 6500, Loss: 0.0136\n",
            "Epoch 9, Batch 7000, Loss: 0.0031\n",
            "Epoch 9, Batch 7500, Loss: 0.0122\n",
            "Epoch 9, Batch 8000, Loss: 0.0184\n",
            "Epoch 9, Batch 8500, Loss: 0.0056\n",
            "Epoch 9, Batch 9000, Loss: 0.0031\n",
            "Epoch 9, Batch 9500, Loss: 0.0188\n",
            "Epoch 9, Batch 10000, Loss: 0.0084\n",
            "Epoch 9, Batch 10500, Loss: 0.0025\n",
            "Epoch 9, Batch 11000, Loss: 0.0055\n",
            "Epoch 9, Batch 11500, Loss: 0.0087\n",
            "Epoch 9, Batch 12000, Loss: 0.0383\n",
            "Epoch 9, Batch 12500, Loss: 0.0123\n",
            "Epoch 9, Batch 13000, Loss: 0.0070\n",
            "Epoch 9, Batch 13500, Loss: 0.0298\n",
            "Epoch 9, Batch 14000, Loss: 0.0108\n",
            "Epoch 9, Batch 14500, Loss: 0.0331\n",
            "Epoch 9, Batch 15000, Loss: 0.0007\n",
            "Epoch 9, Batch 15500, Loss: 0.0116\n",
            "Epoch 9, Batch 16000, Loss: 0.0034\n",
            "Epoch 9, Batch 16500, Loss: 0.0124\n",
            "Epoch 9, Batch 17000, Loss: 0.0812\n",
            "Epoch 9, Batch 17500, Loss: 0.0118\n",
            "Epoch 9, Batch 18000, Loss: 0.0070\n",
            "Epoch 9, Batch 18500, Loss: 0.0113\n",
            "Epoch 9, Batch 19000, Loss: 0.0254\n",
            "Epoch 9, Batch 19500, Loss: 0.0081\n",
            "Epoch 9, Batch 20000, Loss: 0.0074\n",
            "Epoch 9, Batch 20500, Loss: 0.0071\n",
            "Epoch 9, Batch 21000, Loss: 0.0006\n",
            "Epoch 9, Batch 21500, Loss: 0.0125\n",
            "Epoch 9, Batch 22000, Loss: 0.0304\n",
            "Epoch 9, Batch 22500, Loss: 0.0384\n",
            "Epoch 9, Batch 23000, Loss: 0.0057\n",
            "Epoch 9, Batch 23500, Loss: 0.0145\n",
            "Epoch 9, Batch 24000, Loss: 0.0152\n",
            "Epoch 9, Batch 24500, Loss: 0.0707\n",
            "Epoch 9, Batch 25000, Loss: 0.0184\n",
            "Epoch 9, Batch 25500, Loss: 0.0405\n",
            "Epoch 9, Batch 26000, Loss: 0.0217\n",
            "Epoch 9, Batch 26500, Loss: 0.0075\n",
            "Epoch 9, Batch 27000, Loss: 0.0326\n",
            "Epoch 9, Batch 27500, Loss: 0.0050\n",
            "Epoch 9, Batch 28000, Loss: 0.0054\n",
            "Epoch 9, Batch 28500, Loss: 0.0071\n",
            "Epoch 9, Batch 29000, Loss: 0.0303\n",
            "Epoch 9, Batch 29500, Loss: 0.0241\n",
            "Epoch 9, Batch 30000, Loss: 0.0005\n",
            "Epoch 9, Batch 30500, Loss: 0.0062\n",
            "Epoch 9, Batch 31000, Loss: 0.0012\n",
            "Epoch 9, Batch 31500, Loss: 0.0101\n",
            "Epoch 9, Batch 32000, Loss: 0.0322\n",
            "Epoch 9, Batch 32500, Loss: 0.0472\n",
            "Epoch 9, Batch 33000, Loss: 0.0062\n",
            "Epoch 9, Batch 33500, Loss: 0.0213\n",
            "Epoch 9, Batch 34000, Loss: 0.0101\n",
            "Epoch 9, Batch 34500, Loss: 0.0283\n",
            "Epoch 9, Batch 35000, Loss: 0.0166\n",
            "Epoch 9, Batch 35500, Loss: 0.0073\n",
            "Epoch 9, Batch 36000, Loss: 0.0278\n",
            "Epoch 9, Batch 36500, Loss: 0.0187\n",
            "Epoch 9, Batch 37000, Loss: 0.3298\n",
            "Epoch 9, Batch 37500, Loss: 0.0074\n",
            "Epoch 9, Batch 38000, Loss: 0.0525\n",
            "Epoch 9, Batch 38500, Loss: 0.3283\n",
            "Epoch 9, Batch 39000, Loss: 0.0043\n",
            "Epoch 9, Batch 39500, Loss: 0.0104\n",
            "Epoch 9, Batch 40000, Loss: 0.0063\n",
            "Epoch 9, Batch 40500, Loss: 0.0033\n",
            "Epoch 9, Batch 41000, Loss: 0.0769\n",
            "Epoch 9, Batch 41500, Loss: 0.0483\n",
            "Epoch 9, Batch 42000, Loss: 0.0152\n",
            "Epoch 9, Batch 42500, Loss: 0.0077\n",
            "Epoch 9, Batch 43000, Loss: 0.0202\n",
            "Epoch 9, Batch 43500, Loss: 0.0050\n",
            "Epoch 9, Batch 44000, Loss: 0.0477\n",
            "Epoch 9, Batch 44500, Loss: 0.0204\n",
            "Epoch 9, Batch 45000, Loss: 0.0212\n",
            "Epoch 9, Batch 45500, Loss: 0.0063\n",
            "Epoch 9, Batch 46000, Loss: 0.0076\n",
            "Epoch 9, Batch 46500, Loss: 0.0290\n",
            "Epoch 9, Batch 47000, Loss: 0.0076\n",
            "Epoch 9, Batch 47500, Loss: 0.0129\n",
            "Epoch 9, Batch 48000, Loss: 0.0172\n",
            "Epoch 9, Batch 48500, Loss: 0.0425\n",
            "Epoch 9, Batch 49000, Loss: 0.0565\n",
            "Epoch 9, Batch 49500, Loss: 0.0182\n",
            "Epoch 9, Batch 50000, Loss: 0.0083\n",
            "Epoch 9, Batch 50500, Loss: 0.0033\n",
            "Epoch 9, Batch 51000, Loss: 0.0056\n",
            "Epoch 9, Batch 51500, Loss: 0.0031\n",
            "Epoch 9, Batch 52000, Loss: 0.0193\n",
            "Epoch 9, Batch 52500, Loss: 0.0353\n",
            "Epoch 9, Batch 53000, Loss: 0.0478\n",
            "Epoch 9, Batch 53500, Loss: 0.0112\n",
            "Epoch 9, Batch 54000, Loss: 0.0321\n",
            "Epoch 9, Batch 54500, Loss: 0.0148\n",
            "Epoch 9, Batch 55000, Loss: 0.0027\n",
            "Epoch 9, Batch 55500, Loss: 0.0080\n",
            "Epoch 9, Batch 56000, Loss: 0.0289\n",
            "Epoch 9, Batch 56500, Loss: 0.0056\n",
            "Epoch 9, Batch 57000, Loss: 0.0427\n",
            "Epoch 9, Batch 57500, Loss: 0.0620\n",
            "Epoch 9, Batch 58000, Loss: 0.0092\n",
            "Epoch 9, Batch 58500, Loss: 0.0014\n",
            "Epoch 9, Batch 59000, Loss: 0.0582\n",
            "Epoch 9, Batch 59500, Loss: 0.0410\n",
            "Epoch 9, Batch 60000, Loss: 0.0006\n",
            "Epoch 9, Batch 60500, Loss: 0.0153\n",
            "Epoch 9, Batch 61000, Loss: 0.0137\n",
            "Epoch 9, Batch 61500, Loss: 0.0090\n",
            "Epoch 9, Batch 62000, Loss: 0.0374\n",
            "Epoch 9, Batch 62500, Loss: 0.0374\n",
            "Epoch 9, Batch 63000, Loss: 0.0376\n",
            "Epoch 9, Batch 63500, Loss: 0.0107\n",
            "Epoch 9, Batch 64000, Loss: 0.0078\n",
            "Epoch 9, Batch 64500, Loss: 0.0094\n",
            "Epoch 9, Batch 65000, Loss: 0.0137\n",
            "Epoch 9, Batch 65500, Loss: 0.0058\n",
            "Epoch 9, Batch 66000, Loss: 0.0112\n",
            "Epoch 9, Batch 66500, Loss: 0.0037\n",
            "Epoch 9, Batch 67000, Loss: 0.0040\n",
            "Epoch 9, Batch 67500, Loss: 0.0054\n",
            "Epoch 9, Batch 68000, Loss: 0.0974\n",
            "Epoch 9, Batch 68500, Loss: 0.0070\n",
            "Epoch 9, Batch 69000, Loss: 0.0365\n",
            "Epoch 9, Batch 69500, Loss: 0.0014\n",
            "Epoch 9, Batch 70000, Loss: 0.0990\n",
            "Epoch 9, Batch 70500, Loss: 0.0054\n",
            "Epoch 9, Batch 71000, Loss: 0.0056\n",
            "Epoch 9, Batch 71500, Loss: 0.0193\n",
            "Epoch 9, Batch 72000, Loss: 0.0056\n",
            "Epoch 9, Batch 72500, Loss: 0.0125\n",
            "Epoch 9, Batch 73000, Loss: 0.0208\n",
            "Epoch 9, Batch 73500, Loss: 0.0726\n",
            "Epoch 9, Train Loss: 0.0443, Val Loss: 0.1499\n",
            "Epoch 10, Batch 500, Loss: 0.0093\n",
            "Epoch 10, Batch 1000, Loss: 0.0208\n",
            "Epoch 10, Batch 1500, Loss: 0.0169\n",
            "Epoch 10, Batch 2000, Loss: 0.1063\n",
            "Epoch 10, Batch 2500, Loss: 0.0922\n",
            "Epoch 10, Batch 3000, Loss: 0.0349\n",
            "Epoch 10, Batch 3500, Loss: 0.1640\n",
            "Epoch 10, Batch 4000, Loss: 0.0404\n",
            "Epoch 10, Batch 4500, Loss: 0.0454\n",
            "Epoch 10, Batch 5000, Loss: 0.0030\n",
            "Epoch 10, Batch 5500, Loss: 0.0182\n",
            "Epoch 10, Batch 6000, Loss: 0.0199\n",
            "Epoch 10, Batch 6500, Loss: 0.0028\n",
            "Epoch 10, Batch 7000, Loss: 0.0055\n",
            "Epoch 10, Batch 7500, Loss: 0.0109\n",
            "Epoch 10, Batch 8000, Loss: 0.0021\n",
            "Epoch 10, Batch 8500, Loss: 0.0960\n",
            "Epoch 10, Batch 9000, Loss: 0.0071\n",
            "Epoch 10, Batch 9500, Loss: 0.0198\n",
            "Epoch 10, Batch 10000, Loss: 0.0065\n",
            "Epoch 10, Batch 10500, Loss: 0.0117\n",
            "Epoch 10, Batch 11000, Loss: 0.0124\n",
            "Epoch 10, Batch 11500, Loss: 0.0126\n",
            "Epoch 10, Batch 12000, Loss: 0.0014\n",
            "Epoch 10, Batch 12500, Loss: 0.0039\n",
            "Epoch 10, Batch 13000, Loss: 0.0172\n",
            "Epoch 10, Batch 13500, Loss: 0.1463\n",
            "Epoch 10, Batch 14000, Loss: 0.0070\n",
            "Epoch 10, Batch 14500, Loss: 0.0003\n",
            "Epoch 10, Batch 15000, Loss: 0.0131\n",
            "Epoch 10, Batch 15500, Loss: 0.0465\n",
            "Epoch 10, Batch 16000, Loss: 0.0129\n",
            "Epoch 10, Batch 16500, Loss: 0.0097\n",
            "Epoch 10, Batch 17000, Loss: 0.0299\n",
            "Epoch 10, Batch 17500, Loss: 0.0950\n",
            "Epoch 10, Batch 18000, Loss: 0.0329\n",
            "Epoch 10, Batch 18500, Loss: 0.0229\n",
            "Epoch 10, Batch 19000, Loss: 0.0020\n",
            "Epoch 10, Batch 19500, Loss: 0.0183\n",
            "Epoch 10, Batch 20000, Loss: 0.0092\n",
            "Epoch 10, Batch 20500, Loss: 0.0148\n",
            "Epoch 10, Batch 21000, Loss: 0.0298\n",
            "Epoch 10, Batch 21500, Loss: 0.0030\n",
            "Epoch 10, Batch 22000, Loss: 0.0338\n",
            "Epoch 10, Batch 22500, Loss: 0.0354\n",
            "Epoch 10, Batch 23000, Loss: 0.0182\n",
            "Epoch 10, Batch 23500, Loss: 0.0009\n",
            "Epoch 10, Batch 24000, Loss: 0.0079\n",
            "Epoch 10, Batch 24500, Loss: 0.0648\n",
            "Epoch 10, Batch 25000, Loss: 0.0159\n",
            "Epoch 10, Batch 25500, Loss: 0.0328\n",
            "Epoch 10, Batch 26000, Loss: 0.0405\n",
            "Epoch 10, Batch 26500, Loss: 0.1034\n",
            "Epoch 10, Batch 27000, Loss: 0.0258\n",
            "Epoch 10, Batch 27500, Loss: 0.0307\n",
            "Epoch 10, Batch 28000, Loss: 0.0053\n",
            "Epoch 10, Batch 28500, Loss: 0.0023\n",
            "Epoch 10, Batch 29000, Loss: 0.0050\n",
            "Epoch 10, Batch 29500, Loss: 0.0089\n",
            "Epoch 10, Batch 30000, Loss: 0.0132\n",
            "Epoch 10, Batch 30500, Loss: 0.0146\n",
            "Epoch 10, Batch 31000, Loss: 0.0057\n",
            "Epoch 10, Batch 31500, Loss: 0.0096\n",
            "Epoch 10, Batch 32000, Loss: 0.0130\n",
            "Epoch 10, Batch 32500, Loss: 0.0154\n",
            "Epoch 10, Batch 33000, Loss: 0.0151\n",
            "Epoch 10, Batch 33500, Loss: 0.0082\n",
            "Epoch 10, Batch 34000, Loss: 0.0045\n",
            "Epoch 10, Batch 34500, Loss: 0.0027\n",
            "Epoch 10, Batch 35000, Loss: 0.0308\n",
            "Epoch 10, Batch 35500, Loss: 0.1438\n",
            "Epoch 10, Batch 36000, Loss: 0.0299\n",
            "Epoch 10, Batch 36500, Loss: 0.0184\n",
            "Epoch 10, Batch 37000, Loss: 0.0027\n",
            "Epoch 10, Batch 37500, Loss: 0.0048\n",
            "Epoch 10, Batch 38000, Loss: 0.0106\n",
            "Epoch 10, Batch 38500, Loss: 0.0142\n",
            "Epoch 10, Batch 39000, Loss: 0.0296\n",
            "Epoch 10, Batch 39500, Loss: 0.0383\n",
            "Epoch 10, Batch 40000, Loss: 0.0130\n",
            "Epoch 10, Batch 40500, Loss: 0.0222\n",
            "Epoch 10, Batch 41000, Loss: 0.0252\n",
            "Epoch 10, Batch 41500, Loss: 0.0021\n",
            "Epoch 10, Batch 42000, Loss: 0.0178\n",
            "Epoch 10, Batch 42500, Loss: 0.0667\n",
            "Epoch 10, Batch 43000, Loss: 0.0063\n",
            "Epoch 10, Batch 43500, Loss: 0.1894\n",
            "Epoch 10, Batch 44000, Loss: 0.0112\n",
            "Epoch 10, Batch 44500, Loss: 0.0220\n",
            "Epoch 10, Batch 45000, Loss: 0.0075\n",
            "Epoch 10, Batch 45500, Loss: 0.0424\n",
            "Epoch 10, Batch 46000, Loss: 0.0039\n",
            "Epoch 10, Batch 46500, Loss: 0.0041\n",
            "Epoch 10, Batch 47000, Loss: 0.0076\n",
            "Epoch 10, Batch 47500, Loss: 0.0300\n",
            "Epoch 10, Batch 48000, Loss: 0.0113\n",
            "Epoch 10, Batch 48500, Loss: 0.0016\n",
            "Epoch 10, Batch 49000, Loss: 0.0067\n",
            "Epoch 10, Batch 49500, Loss: 0.0115\n",
            "Epoch 10, Batch 50000, Loss: 0.0280\n",
            "Epoch 10, Batch 50500, Loss: 0.0187\n",
            "Epoch 10, Batch 51000, Loss: 0.0055\n",
            "Epoch 10, Batch 51500, Loss: 0.0056\n",
            "Epoch 10, Batch 52000, Loss: 0.0053\n",
            "Epoch 10, Batch 52500, Loss: 0.0094\n",
            "Epoch 10, Batch 53000, Loss: 0.0119\n",
            "Epoch 10, Batch 53500, Loss: 0.0103\n",
            "Epoch 10, Batch 54000, Loss: 0.0141\n",
            "Epoch 10, Batch 54500, Loss: 0.0077\n",
            "Epoch 10, Batch 55000, Loss: 0.0047\n",
            "Epoch 10, Batch 55500, Loss: 0.0454\n",
            "Epoch 10, Batch 56000, Loss: 0.0178\n",
            "Epoch 10, Batch 56500, Loss: 0.0287\n",
            "Epoch 10, Batch 57000, Loss: 0.0013\n",
            "Epoch 10, Batch 57500, Loss: 0.0389\n",
            "Epoch 10, Batch 58000, Loss: 0.0168\n",
            "Epoch 10, Batch 58500, Loss: 0.0050\n",
            "Epoch 10, Batch 59000, Loss: 0.0234\n",
            "Epoch 10, Batch 59500, Loss: 0.0755\n",
            "Epoch 10, Batch 60000, Loss: 0.1005\n",
            "Epoch 10, Batch 60500, Loss: 0.0103\n",
            "Epoch 10, Batch 61000, Loss: 0.0132\n",
            "Epoch 10, Batch 61500, Loss: 0.0068\n",
            "Epoch 10, Batch 62000, Loss: 0.0101\n",
            "Epoch 10, Batch 62500, Loss: 0.0028\n",
            "Epoch 10, Batch 63000, Loss: 0.0034\n",
            "Epoch 10, Batch 63500, Loss: 0.0159\n",
            "Epoch 10, Batch 64000, Loss: 0.0047\n",
            "Epoch 10, Batch 64500, Loss: 0.0124\n",
            "Epoch 10, Batch 65000, Loss: 0.0154\n",
            "Epoch 10, Batch 65500, Loss: 0.0356\n",
            "Epoch 10, Batch 66000, Loss: 0.0235\n",
            "Epoch 10, Batch 66500, Loss: 0.0113\n",
            "Epoch 10, Batch 67000, Loss: 0.0192\n",
            "Epoch 10, Batch 67500, Loss: 0.1128\n",
            "Epoch 10, Batch 68000, Loss: 0.0205\n",
            "Epoch 10, Batch 68500, Loss: 0.0064\n",
            "Epoch 10, Batch 69000, Loss: 0.0113\n",
            "Epoch 10, Batch 69500, Loss: 0.1601\n",
            "Epoch 10, Batch 70000, Loss: 0.0125\n",
            "Epoch 10, Batch 70500, Loss: 0.0219\n",
            "Epoch 10, Batch 71000, Loss: 0.0099\n",
            "Epoch 10, Batch 71500, Loss: 0.0262\n",
            "Epoch 10, Batch 72000, Loss: 0.0308\n",
            "Epoch 10, Batch 72500, Loss: 0.0941\n",
            "Epoch 10, Batch 73000, Loss: 0.0421\n",
            "Epoch 10, Batch 73500, Loss: 0.0060\n",
            "Epoch 10, Train Loss: 0.0434, Val Loss: 0.1427\n",
            "âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ\n",
            "ğŸ¯ ç‰¹å¾å·¥ç¨‹ä¸­...\n",
            "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: 19\n",
            "ğŸ”® å‡†å¤‡æµ‹è¯•æ•°æ®...\n",
            "ç”Ÿæˆé¢„æµ‹...\n",
            "ğŸ‰ å®Œæˆï¼ç”Ÿæˆ 28512 è¡Œé¢„æµ‹ç»“æœ\n",
            "ğŸ“ˆ é¢„æµ‹ç»Ÿè®¡: count    28512.000000\n",
            "mean       440.277935\n",
            "std       1228.727093\n",
            "min          0.000000\n",
            "25%          8.636285\n",
            "50%         31.183722\n",
            "75%        256.952156\n",
            "max      17082.064453\n",
            "Name: sales, dtype: float64\n"
          ]
        }
      ]
    }
  ]
}