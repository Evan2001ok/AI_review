{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJUVGavRtdyj",
        "outputId": "a012c3a9-6620-4b91-de94-7adc182b5356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20250526_035453\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.3.1\n",
            "Python Version:     3.11.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       11.41 GB / 12.67 GB (90.0%)\n",
            "Disk Space Avail:   65.46 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
            "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
            "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
            "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
            "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"/content/AutogluonModels/ag-20250526_035453\"\n",
            "Train Data Rows:    10000\n",
            "Train Data Columns: 18\n",
            "Label Column:       signature\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\tFirst 10 (of 13) unique label values:  [np.int64(-2), np.int64(0), np.int64(2), np.int64(-8), np.int64(4), np.int64(-4), np.int64(-6), np.int64(8), np.int64(6), np.int64(10)]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       multiclass\n",
            "Preprocessing data ...\n",
            "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 9 out of 13 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
            "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9984\n",
            "Train Data Class Count: 9\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11682.91 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.37 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['Symmetry_D8']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 14 | ['chern_simons', 'cusp_volume', 'injectivity_radius', 'longitudinal_translation', 'meridinal_translation_imag', ...]\n",
            "\t\t('int', [])   :  3 | ['Unnamed: 0', 'hyperbolic_adjoint_torsion_degree', 'hyperbolic_torsion_degree']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 9 | ['chern_simons', 'cusp_volume', 'injectivity_radius', 'longitudinal_translation', 'meridinal_translation_imag', ...]\n",
            "\t\t('int', [])       : 3 | ['Unnamed: 0', 'hyperbolic_adjoint_torsion_degree', 'hyperbolic_torsion_degree']\n",
            "\t\t('int', ['bool']) : 5 | ['Symmetry_0', 'Symmetry_D3', 'Symmetry_D4', 'Symmetry_D6', 'Symmetry_Z/2 + Z/2']\n",
            "\t0.3s = Fit runtime\n",
            "\t17 features in original data used to generate 17 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.96 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.44s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8985, Val Rows: 999\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.2232\t = Validation score   (accuracy)\n",
            "\t5.77s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.2132\t = Validation score   (accuracy)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.9409\t = Validation score   (accuracy)\n",
            "\t12.71s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.9459\t = Validation score   (accuracy)\n",
            "\t12.45s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.956\t = Validation score   (accuracy)\n",
            "\t7.27s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.9449\t = Validation score   (accuracy)\n",
            "\t7.35s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.9499\t = Validation score   (accuracy)\n",
            "\t8.48s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.956\t = Validation score   (accuracy)\n",
            "\t71.89s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.9469\t = Validation score   (accuracy)\n",
            "\t3.97s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t2.93s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t0.957\t = Validation score   (accuracy)\n",
            "\t14.12s\t = Training   runtime\n",
            "\t0.38s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.9419\t = Validation score   (accuracy)\n",
            "\t72.1s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9499\t = Validation score   (accuracy)\n",
            "\t15.67s\t = Training   runtime\n",
            "\t0.46s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\tEnsemble Weights: {'RandomForestEntr': 0.25, 'ExtraTreesGini': 0.25, 'KNeighborsUnif': 0.167, 'NeuralNetFastAI': 0.167, 'XGBoost': 0.083, 'NeuralNetTorch': 0.083}\n",
            "\t0.965\t = Validation score   (accuracy)\n",
            "\t0.17s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 242.0s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1292.4 rows/s (999 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/ag-20250526_035453\")\n",
            "Loaded data from: https://raw.githubusercontent.com/mli/ag-docs/main/knot_theory/test.csv | Columns = 19 / 19 | Rows = 5000 -> 5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9478,\n",
              " 'balanced_accuracy': np.float64(0.754478262473782),\n",
              " 'mcc': np.float64(0.9360368834449522)}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#官方例程\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "data_url = 'https://raw.githubusercontent.com/mli/ag-docs/main/knot_theory/'\n",
        "train_data = TabularDataset(f'{data_url}train.csv')\n",
        "train_data.head()\n",
        "\n",
        "label = 'signature'\n",
        "train_data[label].describe()\n",
        "predictor = TabularPredictor(label=label).fit(train_data)\n",
        "\n",
        "test_data = TabularDataset(f'{data_url}test.csv')\n",
        "\n",
        "y_pred = predictor.predict(test_data.drop(columns=[label]))\n",
        "y_pred.head()\n",
        "\n",
        "predictor.evaluate(test_data, silent=True)"
      ]
    }
  ]
}